<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Bilal Khan | Extremely WIP Make your own fast PyTorch-style ML library in Rust</title>
<meta name="description" content="Bilal's Blog
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🤖</text></svg>">

<link rel="stylesheet" href="/blog/assets/css/main.css">
<link rel="canonical" href="/blog/blog/2020/rust-ml-library/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/blog/assets/js/theme.js"></script>
<script src="/blog/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/blog/assets/js/distillpub/template.v2.js"></script>
    <script src="/blog/assets/js/distillpub/transforms.v2.js"></script>
    
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "Extremely WIP Make your own fast PyTorch-style ML library in Rust",
      "description": "",
      "published": "August 2, 2020",
      "authors": [
        
        {
          "author": "Bilal Khan",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://bilal2vec.github.io/blog/">
       Bilal Khan
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/blog/uses">
                /Uses
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>Extremely WIP Make your own fast PyTorch-style ML library in Rust</h1>
        <p></p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <hr />

<p><img alt="A code example showing how my library could be used" src="https://raw.githubusercontent.com/bilal2vec/L2/master/screenshot.png" width="100%" />&lt;/img&gt;</p>

<p align="center">
    <a href="">
        <img src="https://github.com/bilal2vec/l2/workflows/Rust/badge.svg" alt="Rust: CI" />
    </a>
    <a href="https://opensource.org/licenses/MIT">
        <img src="https://img.shields.io/badge/License-MIT-yellow.svg" alt="License: MIT" />
    </a>
    <a href="https://crates.io/crates/l2">
        <img alt="crates.io l2 badge" src="http://meritbadge.herokuapp.com/l2" />
    </a>
    <a href=" https://docs.rs/l2">
        <img alt="docs.rs l2 badge" src="https://docs.rs/l2/badge.svg" />
    </a>
</p>

<hr />

<h1 id="tldr">Tl;dr</h1>

<hr />

<p>This blog post shows you, step-by-step, how to build a fast <a href="https://pytorch.org/">PyTorch</a>-style machine learning library in the <a href="https://www.rust-lang.org/">Rust programming language</a>. This blog post is based on a library called <a href="https://github.com/bilal2vec/L2">L2</a> that I finished working on a while ago.</p>

<p>I <a href="#resources">compiled</a> quite a long list of blog posts, articles, and GitHub repos that I found useful when I was working on L2, so take a look at that if that’s the type of stuff you’re interested in.</p>

<hr />

<p><strong>Disclaimers</strong>:</p>

<p>L2 is a small project I was working on during the summer before uni for fun <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, so don’t expect it to be production-ready or bug-free. <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></p>

<p>I’m going to assume that everyone who’s reading this knows or uses Rust relatively well and is familiar with how PyTorch and TF work at a high level. If you want to learn about these topics or just brush up on some things that you aren’t 💯 clear on, try looking through my <a href="#resources">resources</a> section.</p>

<p>L2 is surprisingly fast especially since I didn’t try very hard to optimize all the operators, it’s usually only less than one order of magnitude slower than PyTorch in most of the benchmarks that I ran. L2 only supports a CPU backend at the moment since I’m not familiar enough with Rust to start working with CUDA and cuDNN. So far, it doesn’t have any PyTorch-style high level abstractions that are really useful for machine learning like PyTorch’s <code class="language-plaintext highlighter-rouge">Parameter</code>, <code class="language-plaintext highlighter-rouge">Layer</code>, or <code class="language-plaintext highlighter-rouge">Module</code> classes. There might still be some bugs in the transpose operators and calling <code class="language-plaintext highlighter-rouge">.backward()</code> on broadcasted tensors. The autograd system won’t automatically clear unused buffers once they’ve been used so this won’t be as memory efficient as PyTorch.</p>

<p>I wrote dozens of tests and benchmarks to make sure that L2 was working properly when I was developing it. I’m going to be omitting tests in this blog post and instead just going to show some example code in <code class="language-plaintext highlighter-rouge">src/bin/main.rs</code>.</p>

<p><strong>If you just want to skip to the code part of the tutorial, click <a href="#baseline">here</a></strong></p>

<h1 id="background">Background</h1>

<hr />

<p>Last summer <sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>, I <a href="https://github.com/bilal2vec/L2/tree/c%2B%2B">wrote</a> a machine learning library as a way of getting better at using C++. The library wasn’t really that advanced (I didn’t have an autograd system like PyTorch does, instead I just did the backprop calculations by hand for each layer) or very fast (I pretty much passed everything by value and didn’t really put a focus on making my code fast and performant), but it was a good way at getting a lot of experience working with a lower level language like c++ that I’d never used before and I learned a lot about how machine learning libraries like Pytorch and Tensorflow work behind the scenes.</p>

<p>This summer, I did a complete rewrite of L2, this time in Rust, with a focus on making it as close to Pytorch as I could (speed and feature wise) and got to learn about and implement a lot of interesting and cool features that are used in all the popular machine learning libraries today.</p>

<p>I’m pretty satisfied with how L2 turned out, here’s the pitch I wrote for it on my GitHub repo:</p>

<p>L2 is a Pytorch-style Tensor+Autograd library written in Rust. It contains a multidimensional array class, <code class="language-plaintext highlighter-rouge">Tensor</code>, with support for strided arrays, numpy-style array slicing, broadcasting, and most major math operations (including fast, BLAS-accelerated matrix multiplication!). On top of this, L2 has a built-in efficient graph-based autograd engine that keeps track of all operations performed on a tensor and topologically sorts and traverses the graph to compute the gradients.</p>

<p>I’m also pretty happy with how the user-facing API of the library turned out:</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">use</span> <span class="nn">l2</span><span class="p">::</span><span class="nn">tensor</span><span class="p">::</span><span class="o">*</span><span class="p">;</span>

<span class="k">let</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">normal</span><span class="p">(</span><span class="o">&amp;</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>
<span class="k">let</span> <span class="n">y</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">normal</span><span class="p">(</span><span class="o">&amp;</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>

<span class="k">let</span> <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span> <span class="o">=</span> <span class="nn">l2</span><span class="p">::</span><span class="nf">matmul</span><span class="p">(</span><span class="o">&amp;</span><span class="n">x</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">y</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>

<span class="n">z</span><span class="nf">.backward</span><span class="p">();</span>

<span class="nd">println!</span><span class="p">(</span><span class="s">"{}"</span><span class="p">,</span> <span class="n">z</span><span class="p">);</span>
</code></pre></div></div>

<hr />

<h1 id="lets-get-started">Let’s get started</h1>

<hr />

<p>So let’s get started. I’m pretty much just copying down the installation instructions from the official <a href="https://www.rust-lang.org/learn/get-started">get started</a> guide, so take a look at that if you want.</p>

<p>Install rustup to your computer:</p>

<hr />

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bilal@Bilals-MacBook-Pro ~ % curl <span class="nt">--proto</span> <span class="s1">'=https'</span> <span class="nt">--tlsv1</span>.2 <span class="nt">-sSf</span> https://sh.rustup.rs | sh
</code></pre></div></div>

<hr />

<p>Switch the default rust version to nightly:</p>

<hr />

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bilal@Bilals-MacBook-Pro ~ % rustup default nightly
</code></pre></div></div>

<hr />

<p>I’ll be using my preferred text editor <a href="https://code.visualstudio.com/">VScode</a> in this post, but feel free to use whatever editor you prefer.</p>

<p>I highly recommend using the (soon to become) official Rust extension for VScode, <a href="https://marketplace.visualstudio.com/items?itemName=matklad.rust-analyzer">Rust-analyzer</a> instead of the old <a href="https://marketplace.visualstudio.com/items?itemName=rust-lang.rust">RLS</a> extension. Just install it from the marketplace and you should be ready to go.</p>

<p>Create a new Rust library called <code class="language-plaintext highlighter-rouge">l2</code> with:</p>

<hr />

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bilal@Bilals-MacBook-Pro ~ % cargo new l2 <span class="nt">--lib</span>
</code></pre></div></div>

<hr />

<p>Install clippy (Rust’s official linter):</p>

<p><em>You can take a look at all the lint rules and how to fix each one <a href="https://rust-lang.github.io/rust-clippy/master/index.html">here</a></em></p>

<hr />

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bilal@Bilals-MacBook-Pro ~ % rustup component add clippy
</code></pre></div></div>

<hr />

<p>And change rust-analyzer to use clippy as its default linter by creating a <code class="language-plaintext highlighter-rouge">.vscode/settings.json</code> file and pasting this in it.</p>

<hr />

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
	</span><span class="nl">"rust-analyzer.checkOnSave.command"</span><span class="p">:</span><span class="w"> </span><span class="s2">"clippy"</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<hr />

<p>For debugging support, I use the <a href="https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb">Code-LLDB</a> extension, so install that as well.</p>

<p>create a <code class="language-plaintext highlighter-rouge">.vscode/launch.json</code> file and paste this into it:</p>

<hr />

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
	</span><span class="nl">"version"</span><span class="p">:</span><span class="w"> </span><span class="s2">"0.2.0"</span><span class="p">,</span><span class="w">
	</span><span class="nl">"configurations"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
		</span><span class="p">{</span><span class="w">
			</span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"lldb"</span><span class="p">,</span><span class="w">
			</span><span class="nl">"request"</span><span class="p">:</span><span class="w"> </span><span class="s2">"launch"</span><span class="p">,</span><span class="w">
			</span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Debug"</span><span class="p">,</span><span class="w">
			</span><span class="nl">"program"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${workspaceRoot}/target/debug/main"</span><span class="p">,</span><span class="w">
			</span><span class="nl">"args"</span><span class="p">:</span><span class="w"> </span><span class="p">[],</span><span class="w">
			</span><span class="nl">"cwd"</span><span class="p">:</span><span class="w"> </span><span class="s2">"${workspaceRoot}"</span><span class="w">
		</span><span class="p">}</span><span class="w">
	</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<hr />

<p>Add a rust binary target <code class="language-plaintext highlighter-rouge">src/bin/main.rs</code> that will be linked against our library at <code class="language-plaintext highlighter-rouge">src/lib.rs</code>. Your project should now have a directory structure like this:</p>

<hr />

<pre><code class="language-plain">.git/
.vscode/
    settings.json
    launch.json
src/
    bin/
        main.rs
    lib.rs
target/
.gitignore
Cargo.lock
Cargo.toml
</code></pre>

<hr />

<p>We’ll code up the library in <code class="language-plaintext highlighter-rouge">src/lib.rs</code> and any other files in the <code class="language-plaintext highlighter-rouge">src/</code> directory. We’ll use <code class="language-plaintext highlighter-rouge">src/bin/main.rs</code> to interact with L2 as you would when using it in your own project.</p>

<hr />

<h1 id="a-simple-baseline">A simple baseline</h1>

<hr />

<p>Ok, so let’s start by creating a simple <code class="language-plaintext highlighter-rouge">Tensor</code> struct and defining a few simple operations on it.</p>

<p>A <code class="language-plaintext highlighter-rouge">Tensor</code> is really just a multidimensional array. For this library, we’ll keep it simple and restrict tensors to have at most $2$ dimensions (You’ll see why later).</p>

<p>The simplest way to store a multidimensional array of say, dimensions $m \times n$ would be to create an array of length $m$ that holds a pointers to $m$ distinct arrays of length $n$, each holding the elements of a single row. This would be the simplest way to represent a <code class="language-plaintext highlighter-rouge">Tensor</code> but isn’t really optimal when you need to create and process large <code class="language-plaintext highlighter-rouge">Tensors</code> quickly.</p>

<p>Most (if not all) people use <em>strided arrays</em>, where elements of a multidimensional array are layed out contigously in memory (the $m \times n$ <code class="language-plaintext highlighter-rouge">Tensor</code> would then be represented as single array of length $m * n$).</p>

<p>Take a look at http://blog.ezyang.com/2019/05/pytorch-internals/ for a good in-depth look into how PyTorch uses strided arrays. I’ll summarize the main parts below:</p>

<p>Say you have a $2 \times 2$ <code class="language-plaintext highlighter-rouge">Tensor</code> like this:</p>

\[\begin{bmatrix}
   1 &amp; 2 \\
   3 &amp; 4
\end{bmatrix}\]

<p>If you wanted to represent this as a strided array, you could either store them in row-major or column-major order, storing either values from a single row or column contigously in memory (the same idea would still apply if you have a <code class="language-plaintext highlighter-rouge">Tensor</code> of more dimensions):</p>

\[\text {row-major:}
\begin{bmatrix}
   1, 2, 3, 4
\end{bmatrix}\]

\[\text {column-major:}
\begin{bmatrix}
   1, 3, 2, 4
\end{bmatrix}\]

<p>Most machine learning libraries like Numpy, PyTorch, and Tensorflow store Tensors in row-major order by default. This lets you quickly access the next element in the same row just by moving one element to the right in the <code class="language-plaintext highlighter-rouge">Tensor</code>. Column-major order isn’t as commonly used, the only time I had to use it when I was integrating a BLAS library written in very optimized Fortran into L2 in order to use its super fast matrix multiplication implementations (using BLAS sped up my matrix multiplication code by about 200 times IIRC).</p>

<p>The choice of whether to store your data in column-major or row-major order depends on whether you prefer to have contigous access to elements in the first or last dimensions of your <code class="language-plaintext highlighter-rouge">Tensor</code>. For example, if you store a batch of $N$ three-channel image in a <code class="language-plaintext highlighter-rouge">Tensor</code> of dimensions ($256$, $256$, $3$), you would be able to either access the channels or the batch dimension contigously (i.e. have the elements in that dimension be next to each other in memory) depending on whether it’s stored in row-major or column-major order.</p>

<p>The <em>stride</em> for each dimension of a <em>strided array</em> is the number of elements you want to skip between neighboring elements of a <code class="language-plaintext highlighter-rouge">Tensor</code> in a particular dimension. For example, our original <code class="language-plaintext highlighter-rouge">Tensor</code> of shape $\begin{bmatrix} 2, 2 \end{bmatrix}$ has strides of $\begin{bmatrix} 2, 1 \end{bmatrix}$.</p>

<p>This means that if we want to advance one element in the column dimension (from the element $1$ to the element $3$) of the <em>logical</em> <code class="language-plaintext highlighter-rouge">Tensor</code>, we need to advance $2$ elements at a time in the <em>strided</em> <code class="language-plaintext highlighter-rouge">Tensor</code>.</p>

\[\text {Logical Tensor:}

\begin{bmatrix}
   1 &amp; \color{gray} 2 \\
   3 &amp; \color{gray} 4
\end{bmatrix}\]

\[\text {Strided Tensor:}
\begin{bmatrix}
   1, \color{gray} 2, \color{white} 3, \color{gray} 4
\end{bmatrix}\]

<p>The same would be true for the other dimensions as well. If we want to advance one element in the row dimension (from the element $1$ to the element $2$) of the <em>logical</em> <code class="language-plaintext highlighter-rouge">Tensor</code>, we would advance $1$ element in the <em>strided</em> <code class="language-plaintext highlighter-rouge">Tensor</code>.</p>

\[\text {Logical Tensor:}

\begin{bmatrix}
   1 &amp;  2 \\
   \color{gray} 3 &amp; \color{gray} 4
\end{bmatrix}\]

\[\text {Strided Tensor:}
\begin{bmatrix}
   1,  2, \color{gray} 3, 4
\end{bmatrix}\]

<p>If we wanted to get the <em>physical</em> location in memory of a specific element in the <code class="language-plaintext highlighter-rouge">Tensor</code> from the <em>logical</em> location, we can simply “multiply each index with the respective stride for that dimension, and sum them all together” <sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>. So for an example, if we want to get the <em>physical</em> index of the element at the <em>logical</em> indices $[ 1, 1]$, we would calculate it like this:</p>

\[\text{logical index: } [\color{red} 1, \color{blue} 1 \color{white}] \ \text{strides: } [\color{red} 2, \color{blue} 1 \color{white}]\]

<p><br /></p>

\[\text{physical index} = {\color{blue} 1} {\color{white} *} {\color{blue} 1} {\color{white} +} {\color{red} 1} {\color{white} *} {\color{red} 2}\]

\[\text{physical index} =  1 + 2 = 3\]

\[\text {element at [1, 1]} =
\begin{bmatrix}
   \color{gray} 1,  2,  3, \color{white} 4
\end{bmatrix}\]

<hr />

<p>So now that we have that out of the way, let’s start writing some code.</p>

<p>In this section, we’ll make a basic <code class="language-plaintext highlighter-rouge">Tensor</code> struct the just creates and stores a strided array. We’ll also take advantage of Rust’s excellent error handling primitives to add robust error handling and add pretty printing of our <code class="language-plaintext highlighter-rouge">Tensors</code>.</p>

<p>Let’s make a new file at <code class="language-plaintext highlighter-rouge">src/tensor.rs</code> to house our <code class="language-plaintext highlighter-rouge">Tensor</code> struct.</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">use</span> <span class="nn">crate</span><span class="p">::</span><span class="nn">errors</span><span class="p">::</span><span class="n">TensorError</span><span class="p">;</span>

<span class="k">use</span> <span class="nn">std</span><span class="p">::</span><span class="n">fmt</span><span class="p">;</span>

<span class="nd">#[derive(Debug,</span> <span class="nd">PartialEq)]</span>
<span class="k">pub</span> <span class="k">struct</span> <span class="n">Tensor</span> <span class="p">{</span>
    <span class="k">pub</span> <span class="n">data</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="k">pub</span> <span class="n">shape</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="k">pub</span> <span class="n">strides</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span><span class="p">,</span>
<span class="p">}</span>

<span class="c">// Pretty print Tensors</span>
<span class="k">impl</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Display</span> <span class="k">for</span> <span class="n">Tensor</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">fmt</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Formatter</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Result</span> <span class="p">{</span>
        <span class="k">let</span> <span class="n">graph</span> <span class="o">=</span> <span class="nd">format!</span><span class="p">(</span><span class="s">"Value: {:?} </span><span class="se">\n</span><span class="s">Shape: {:?}"</span><span class="p">,</span>
            <span class="k">self</span><span class="py">.data</span><span class="p">,</span> <span class="k">self</span><span class="py">.shape</span><span class="p">);</span>

        <span class="nd">write!</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"{}"</span><span class="p">,</span> <span class="n">graph</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="n">Clone</span> <span class="k">for</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">clone</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Self</span> <span class="p">{</span>
        <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="k">self</span><span class="py">.data</span><span class="nf">.clone</span><span class="p">(),</span> <span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">()</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="n">Tensor</span> <span class="p">{</span>
    <span class="c">// Calculate the number of elements in a tensor from the shape</span>
    <span class="k">fn</span> <span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">usize</span><span class="p">])</span> <span class="k">-&gt;</span> <span class="nb">usize</span> <span class="p">{</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">length</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="n">in</span> <span class="n">shape</span> <span class="p">{</span>
            <span class="n">length</span> <span class="o">*=</span> <span class="n">i</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="n">length</span>
    <span class="p">}</span>

    <span class="c">// calculate the strides for each dimension from the shape</span>
    <span class="k">fn</span> <span class="nf">calc_strides_from_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">usize</span><span class="p">])</span> <span class="k">-&gt;</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">strides</span> <span class="o">=</span> <span class="nn">Vec</span><span class="p">::</span><span class="nf">with_capacity</span><span class="p">(</span><span class="n">shape</span><span class="nf">.len</span><span class="p">());</span>

        <span class="k">let</span> <span class="k">mut</span> <span class="n">current_stride</span> <span class="o">=</span> <span class="mi">1</span><span class="p">;</span>
        <span class="k">for</span> <span class="n">i</span> <span class="n">in</span> <span class="n">shape</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.rev</span><span class="p">()</span> <span class="p">{</span>
            <span class="n">strides</span><span class="nf">.insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">current_stride</span><span class="p">);</span>
            <span class="n">current_stride</span> <span class="o">*=</span> <span class="n">i</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="n">strides</span>
    <span class="p">}</span>

    <span class="c">// Create a new tensor from some data with a specific shape</span>
    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">new</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">usize</span><span class="p">])</span> <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="k">if</span> <span class="n">data</span><span class="nf">.len</span><span class="p">()</span> <span class="o">==</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
            <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">shape</span><span class="nf">.is_empty</span><span class="p">()</span>
            <span class="o">&amp;&amp;</span> <span class="n">shape</span><span class="nf">.len</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">3</span>
        <span class="p">{</span>
            <span class="nf">Ok</span><span class="p">(</span><span class="n">Tensor</span> <span class="p">{</span>
                <span class="n">data</span><span class="p">,</span>
                <span class="n">shape</span><span class="p">:</span> <span class="n">shape</span><span class="nf">.to_vec</span><span class="p">(),</span>
                <span class="n">strides</span><span class="p">:</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_strides_from_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span>
            <span class="p">})</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="nf">Err</span><span class="p">(</span><span class="nn">TensorError</span><span class="p">::</span><span class="n">InvalidTensor</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>Let’s add the error handling struct <code class="language-plaintext highlighter-rouge">TensorError</code> to <code class="language-plaintext highlighter-rouge">src/errors.rs</code></p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/errors.rs</span>

<span class="k">use</span> <span class="nn">std</span><span class="p">::</span><span class="n">error</span><span class="p">;</span>
<span class="k">use</span> <span class="nn">std</span><span class="p">::</span><span class="n">fmt</span><span class="p">;</span>

<span class="nd">#[derive(Debug,</span> <span class="nd">Clone)]</span>
<span class="k">pub</span> <span class="k">enum</span> <span class="n">TensorError</span> <span class="p">{</span>
    <span class="n">MaxDimsError</span><span class="p">,</span>
    <span class="n">InvalidTensor</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Display</span> <span class="k">for</span> <span class="n">TensorError</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">fmt</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Formatter</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Result</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span> <span class="p">{</span>
            <span class="nn">TensorError</span><span class="p">::</span><span class="n">MaxDimsError</span> <span class="k">=&gt;</span> <span class="nd">write!</span><span class="p">(</span>
                <span class="n">f</span><span class="p">,</span>
                <span class="s">"L2 currently only supports
                tensors with up to 2 dimensions"</span>
            <span class="p">),</span>
            <span class="nn">TensorError</span><span class="p">::</span><span class="n">InvalidTensor</span> <span class="k">=&gt;</span>
            <span class="nd">write!</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"Invalid parameters for Tensor"</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="c">// This is important for other errors to wrap this one.</span>
<span class="k">impl</span> <span class="nn">error</span><span class="p">::</span><span class="n">Error</span> <span class="k">for</span> <span class="n">TensorError</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">source</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nb">Option</span><span class="o">&lt;&amp;</span><span class="p">(</span><span class="n">dyn</span> <span class="nn">error</span><span class="p">::</span><span class="n">Error</span> <span class="o">+</span> <span class="nv">'static</span><span class="p">)</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="c">// Generic error, underlying cause isn't tracked.</span>
        <span class="nb">None</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>Add the relevant imports to <code class="language-plaintext highlighter-rouge">src/lib.rs</code> and <code class="language-plaintext highlighter-rouge">src/bin/main.rs</code></p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/lib.rs</span>

<span class="k">pub</span> <span class="k">mod</span> <span class="n">errors</span><span class="p">;</span>
<span class="k">pub</span> <span class="k">mod</span> <span class="n">tensor</span><span class="p">;</span>
</code></pre></div></div>

<hr />

<p>And finally, lets test out our library by creating a simple <code class="language-plaintext highlighter-rouge">Tensor</code> in <code class="language-plaintext highlighter-rouge">src/bin/main.rs</code></p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/bin/main.rs</span>

<span class="k">use</span> <span class="nn">l2</span><span class="p">::</span><span class="nn">errors</span><span class="p">::</span><span class="o">*</span><span class="p">;</span>
<span class="k">use</span> <span class="nn">l2</span><span class="p">::</span><span class="nn">tensor</span><span class="p">::</span><span class="o">*</span><span class="p">;</span>

<span class="k">fn</span> <span class="nf">main</span><span class="p">()</span> <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="p">(),</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="k">let</span> <span class="n">x</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="o">&amp;</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span><span class="o">?</span><span class="p">;</span>

    <span class="nd">println!</span><span class="p">(</span><span class="s">"{}"</span><span class="p">,</span> <span class="n">x</span><span class="p">);</span>

    <span class="nf">Ok</span><span class="p">(())</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>and run <code class="language-plaintext highlighter-rouge">cargo run</code> to see the output.</p>

<hr />

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bilal@Bilals-MacBook-Pro l2 % cargo run
   Compiling l2 v0.1.0 <span class="o">(</span>/Users/bilal/Desktop/l2<span class="o">)</span>
    Finished dev <span class="o">[</span>unoptimized + debuginfo] target<span class="o">(</span>s<span class="o">)</span> <span class="k">in </span>1.02s
     Running <span class="sb">`</span>target/debug/main<span class="sb">`</span>

Value: <span class="o">[</span>1.0, 2.0, 3.0]
Shape: <span class="o">[</span>3]
</code></pre></div></div>

<hr />

<p>🎉! you now have a very simple machine learning library. Now that we have the general structure of the library set up, I'll be speeding up the pace of this blog post.</p>

<hr />

<h1 id="broadcasting">Broadcasting</h1>

<hr />

<p>Storing a bunch of values in a <code class="language-plaintext highlighter-rouge">Tensor</code> is useless if we can’t operate over them.</p>

<p>Before we can create some <code class="language-plaintext highlighter-rouge">Tensor</code>—<code class="language-plaintext highlighter-rouge">Tensor</code> operations, we need to implement <em>broadcasting</em>. I won’t go into what exactly broadcasting is here, since there are a lot of better explanations out there. Numpy’s <a href="https://numpy.org/doc/stable/user/basics.broadcasting.html#general-broadcasting-rules">documentation</a> on their broadcasting rules is a good technical explanation.</p>

<p>One thing the numpy docs don’t go into is how to implement broadcasting. I struggled with how to best implement it when I was making my original C++ version of the library last year <sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>, but I eventually settled on the pretty simple and efficient solution of adding dimensions of size $1$ to the tensor with the fewer number of dimensions to make their shapes broadcastable, then setting the shapes and strides of a broadcasted dimension to $1$ and $0$ respectively. By doing it this way, the <code class="language-plaintext highlighter-rouge">Tensor</code> would use the same value across all values of a specific dimension.</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/tensor.rs</span>

<span class="k">impl</span> <span class="n">Tensor</span> <span class="p">{</span>

    <span class="o">...</span>

    <span class="nd">#[allow(clippy::ptr_arg,</span> <span class="nd">clippy::type_complexity)]</span>
    <span class="k">fn</span> <span class="nf">broadcast</span><span class="p">(</span>
        <span class="n">lhs_shape</span><span class="p">:</span> <span class="o">&amp;</span><span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span><span class="p">,</span>
        <span class="n">rhs_shape</span><span class="p">:</span> <span class="o">&amp;</span><span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="p">(</span><span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span><span class="p">,</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span><span class="p">,</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span><span class="p">),</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>

        <span class="c">// prepend lhs_shape with ones if the length of it is smaller than rhs_shape</span>
        <span class="k">let</span> <span class="n">lhs_shape</span> <span class="o">=</span> <span class="k">if</span> <span class="n">lhs_shape</span><span class="nf">.len</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">rhs_shape</span><span class="nf">.len</span><span class="p">()</span> <span class="p">{</span>
            <span class="k">let</span> <span class="n">ones</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span><span class="mi">1</span><span class="p">;</span> <span class="n">rhs_shape</span><span class="nf">.len</span><span class="p">()</span> <span class="o">-</span> <span class="n">lhs_shape</span><span class="nf">.len</span><span class="p">()];</span>
            <span class="p">[</span><span class="o">&amp;</span><span class="n">ones</span><span class="p">[</span><span class="o">..</span><span class="p">],</span> <span class="o">&amp;</span><span class="n">lhs_shape</span><span class="p">[</span><span class="o">..</span><span class="p">]]</span><span class="nf">.concat</span><span class="p">()</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">lhs_shape</span><span class="nf">.clone</span><span class="p">()</span>
        <span class="p">};</span>

        <span class="c">// prepend rhs_shape with ones if the length of it is smaller than lhs_shape</span>
        <span class="k">let</span> <span class="n">rhs_shape</span> <span class="o">=</span> <span class="k">if</span> <span class="n">rhs_shape</span><span class="nf">.len</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">lhs_shape</span><span class="nf">.len</span><span class="p">()</span> <span class="p">{</span>
            <span class="k">let</span> <span class="n">ones</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span><span class="mi">1</span><span class="p">;</span> <span class="n">lhs_shape</span><span class="nf">.len</span><span class="p">()</span> <span class="o">-</span> <span class="n">rhs_shape</span><span class="nf">.len</span><span class="p">()];</span>
            <span class="p">[</span><span class="o">&amp;</span><span class="n">ones</span><span class="p">[</span><span class="o">..</span><span class="p">],</span> <span class="o">&amp;</span><span class="n">rhs_shape</span><span class="p">[</span><span class="o">..</span><span class="p">]]</span><span class="nf">.concat</span><span class="p">()</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="n">rhs_shape</span><span class="nf">.clone</span><span class="p">()</span>
        <span class="p">};</span>

        <span class="k">let</span> <span class="k">mut</span> <span class="n">broadcasted_shape</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span> <span class="o">=</span>
            <span class="nn">Vec</span><span class="p">::</span><span class="nf">with_capacity</span><span class="p">(</span><span class="n">lhs_shape</span><span class="nf">.len</span><span class="p">());</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">broadcasted_lhs_strides</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span> <span class="o">=</span>
            <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_strides_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="n">lhs_shape</span><span class="p">);</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">broadcasted_rhs_strides</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">usize</span><span class="o">&gt;</span> <span class="o">=</span>
            <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_strides_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="n">rhs_shape</span><span class="p">);</span>

        <span class="c">// go over each dimension of lhs and rhs</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="o">&amp;</span><span class="n">lhs</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rhs</span><span class="p">))</span> <span class="n">in</span> <span class="n">lhs_shape</span><span class="nf">.iter</span><span class="p">()</span>
            <span class="nf">.zip</span><span class="p">(</span><span class="n">rhs_shape</span><span class="nf">.iter</span><span class="p">())</span><span class="nf">.enumerate</span><span class="p">()</span> <span class="p">{</span>
            <span class="c">// if both dimensions are the same,</span>
            <span class="c">// the dimension of the broadcasted shape</span>
            <span class="c">// for this dimension doesn't change</span>
            <span class="k">if</span> <span class="n">lhs</span> <span class="o">==</span> <span class="n">rhs</span> <span class="p">{</span>
                <span class="n">broadcasted_shape</span><span class="nf">.push</span><span class="p">(</span><span class="n">lhs</span><span class="p">);</span>

            <span class="c">// if the size of this dimension of lhs</span>
            <span class="c">// is 1, set the strides of lhs for that</span>
            <span class="c">// dimension to 0</span>
            <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="n">lhs</span> <span class="o">==</span> <span class="mi">1</span> <span class="p">{</span>
                <span class="n">broadcasted_shape</span><span class="nf">.push</span><span class="p">(</span><span class="n">rhs</span><span class="p">);</span>
                <span class="n">broadcasted_lhs_strides</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

            <span class="c">// if the size of this dimension of rhs</span>
            <span class="c">// is 1, set the strides of rhs for</span>
            <span class="c">// that dimension to 0</span>
            <span class="p">}</span> <span class="k">else</span> <span class="k">if</span> <span class="n">rhs</span> <span class="o">==</span> <span class="mi">1</span> <span class="p">{</span>
                <span class="n">broadcasted_shape</span><span class="nf">.push</span><span class="p">(</span><span class="n">lhs</span><span class="p">);</span>
                <span class="n">broadcasted_rhs_strides</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

            <span class="c">// return an error if the tensors</span>
            <span class="c">// aren't broadcastable</span>
            <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                <span class="k">return</span> <span class="nf">Err</span><span class="p">(</span><span class="nn">TensorError</span><span class="p">::</span><span class="n">BroadcastError</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">}</span>

        <span class="nf">Ok</span><span class="p">((</span>
            <span class="n">broadcasted_shape</span><span class="p">,</span>
            <span class="n">broadcasted_lhs_strides</span><span class="p">,</span>
            <span class="n">broadcasted_rhs_strides</span><span class="p">,</span>
        <span class="p">))</span>
    <span class="p">}</span>
</code></pre></div></div>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/errors.rs</span>

<span class="k">pub</span> <span class="k">enum</span> <span class="n">TensorError</span> <span class="p">{</span>
    <span class="o">...</span>
    <span class="n">BroadcastError</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Display</span> <span class="k">for</span> <span class="n">TensorError</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">fmt</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Formatter</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Result</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span> <span class="p">{</span>
            <span class="o">...</span>
            <span class="nn">TensorError</span><span class="p">::</span><span class="n">BroadcastError</span> <span class="k">=&gt;</span>
                <span class="nd">write!</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"Shapes are not broadcastable"</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>Now that we’ve implemented broadcasting, we’ll add some operations over <code class="language-plaintext highlighter-rouge">Tensor</code>s in the next section so we can try it out.</p>

<hr />

<h1 id="ops">Ops</h1>

<hr />

<p>Let’s start by defining a struct <code class="language-plaintext highlighter-rouge">Ops</code> that we’ll use to keep track of what operation should be performed on a tensor.</p>

<p>We’ll be storing the <code class="language-plaintext highlighter-rouge">Tensor</code>—<code class="language-plaintext highlighter-rouge">Tensor</code> ops in an enum called <code class="language-plaintext highlighter-rouge">TensorOp</code>, but we’ll wrap that in the <code class="language-plaintext highlighter-rouge">Ops</code> enum so we can add more different kinds of ops in the future (slicing, matmuls, transposes, etc).</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/ops.rs</span>

<span class="k">use</span> <span class="nn">std</span><span class="p">::</span><span class="n">fmt</span><span class="p">;</span>

<span class="nd">#[derive(Debug,</span> <span class="nd">PartialEq)]</span>
<span class="k">pub</span> <span class="k">enum</span> <span class="n">TensorOp</span> <span class="p">{</span>
    <span class="nb">Add</span><span class="p">,</span>
    <span class="nb">Sub</span><span class="p">,</span>
    <span class="nb">Mul</span><span class="p">,</span>
    <span class="nb">Div</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Display</span> <span class="k">for</span> <span class="n">TensorOp</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">fmt</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Formatter</span><span class="o">&lt;</span><span class="nv">'_</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Result</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span> <span class="p">{</span>
            <span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Add</span> <span class="k">=&gt;</span> <span class="nd">write!</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"Add"</span><span class="p">),</span>
            <span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Sub</span> <span class="k">=&gt;</span> <span class="nd">write!</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"Subtract"</span><span class="p">),</span>
            <span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Mul</span> <span class="k">=&gt;</span> <span class="nd">write!</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"Multiply"</span><span class="p">),</span>
            <span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Div</span> <span class="k">=&gt;</span> <span class="nd">write!</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"Divide"</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="nd">#[derive(Debug,</span> <span class="nd">PartialEq)]</span>
<span class="k">pub</span> <span class="k">enum</span> <span class="n">Ops</span> <span class="p">{</span>
    <span class="nf">TensorOp</span><span class="p">(</span><span class="n">TensorOp</span><span class="p">),</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Display</span> <span class="k">for</span> <span class="n">Ops</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">fmt</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Formatter</span><span class="o">&lt;</span><span class="nv">'_</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Result</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span> <span class="p">{</span>
            <span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="n">tensor_op</span><span class="p">)</span> <span class="k">=&gt;</span>
                <span class="nd">write!</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"{}"</span><span class="p">,</span> <span class="n">tensor_op</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>And now let’s add an <code class="language-plaintext highlighter-rouge">OpError</code> variant to our <code class="language-plaintext highlighter-rouge">TensorError</code> enum</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/errors.rs</span>

<span class="k">pub</span> <span class="k">enum</span> <span class="n">TensorError</span> <span class="p">{</span>
    <span class="o">...</span>
    <span class="n">OpError</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Display</span> <span class="k">for</span> <span class="n">TensorError</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">fmt</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Formatter</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Result</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span> <span class="p">{</span>
            <span class="o">...</span>
            <span class="nn">TensorError</span><span class="p">::</span><span class="n">OpError</span> <span class="k">=&gt;</span>
                <span class="nd">write!</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"Tensors cannot be operated on"</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>

</code></pre></div></div>

<hr />

<p>Now that we have an <code class="language-plaintext highlighter-rouge">Ops</code> enum that we can use, let’s integrate it into <code class="language-plaintext highlighter-rouge">Tensor</code></p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/tensor.rs</span>

<span class="k">use</span> <span class="nn">std</span><span class="p">::</span><span class="nn">ops</span><span class="p">::{</span><span class="nb">Add</span><span class="p">,</span> <span class="nb">Div</span><span class="p">,</span> <span class="nb">Mul</span><span class="p">,</span> <span class="nb">Sub</span><span class="p">};</span>

<span class="k">use</span> <span class="nn">crate</span><span class="p">::</span><span class="nn">ops</span><span class="p">::{</span><span class="n">Ops</span><span class="p">,</span> <span class="n">TensorOp</span><span class="p">};</span>

<span class="k">impl</span> <span class="n">Tensor</span> <span class="p">{</span>

    <span class="o">...</span>

    <span class="c">// calculate the physical index of an element</span>
    <span class="c">// from a `Tensor`'s logical indices and strides</span>
    <span class="k">fn</span> <span class="nf">get_physical_idx</span><span class="p">(</span><span class="n">logical_indices</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">usize</span><span class="p">],</span>
        <span class="n">strides</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">usize</span><span class="p">])</span> <span class="k">-&gt;</span> <span class="nb">usize</span> <span class="p">{</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">physical_idx</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>

        <span class="k">for</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="n">in</span> <span class="n">logical_indices</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.enumerate</span><span class="p">()</span> <span class="p">{</span>
            <span class="n">physical_idx</span> <span class="o">+=</span> <span class="n">idx</span> <span class="o">*</span> <span class="n">strides</span><span class="p">[</span><span class="n">i</span><span class="p">];</span>
        <span class="p">}</span>

        <span class="n">physical_idx</span>
    <span class="p">}</span>

    <span class="c">// perform op on lhs and rhs</span>
    <span class="k">fn</span> <span class="nf">op</span><span class="p">(</span><span class="n">lhs</span><span class="p">:</span> <span class="o">&amp;</span><span class="nb">f32</span><span class="p">,</span> <span class="n">rhs</span><span class="p">:</span> <span class="o">&amp;</span><span class="nb">f32</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Ops</span><span class="p">)</span> <span class="k">-&gt;</span>
        <span class="n">Result</span><span class="o">&lt;</span><span class="nb">f32</span><span class="p">,</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="k">match</span> <span class="n">op</span> <span class="p">{</span>
            <span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Add</span><span class="p">)</span> <span class="k">=&gt;</span>
                <span class="nf">Ok</span><span class="p">(</span><span class="n">lhs</span> <span class="o">+</span> <span class="n">rhs</span><span class="p">),</span>
            <span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Sub</span><span class="p">)</span> <span class="k">=&gt;</span>
                <span class="nf">Ok</span><span class="p">(</span><span class="n">lhs</span> <span class="o">-</span> <span class="n">rhs</span><span class="p">),</span>
            <span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Mul</span><span class="p">)</span> <span class="k">=&gt;</span>
                <span class="nf">Ok</span><span class="p">(</span><span class="n">lhs</span> <span class="o">*</span> <span class="n">rhs</span><span class="p">),</span>
            <span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Div</span><span class="p">)</span> <span class="k">=&gt;</span>
                <span class="nf">Ok</span><span class="p">(</span><span class="n">lhs</span> <span class="o">/</span> <span class="n">rhs</span><span class="p">),</span>
            <span class="mi">_</span> <span class="k">=&gt;</span> <span class="nf">Err</span><span class="p">(</span><span class="nn">TensorError</span><span class="p">::</span><span class="n">OpError</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>

    <span class="k">fn</span> <span class="nf">tensor_op</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">Ops</span><span class="p">)</span> <span class="k">-&gt;</span>
        <span class="n">Result</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="c">// broadcast tensors</span>
        <span class="k">let</span> <span class="p">(</span><span class="n">new_shape</span><span class="p">,</span> <span class="n">lhs_strides</span><span class="p">,</span> <span class="n">rhs_strides</span><span class="p">)</span> <span class="o">=</span>
            <span class="nn">Tensor</span><span class="p">::</span><span class="nf">broadcast</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">other</span><span class="py">.shape</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>

        <span class="k">if</span> <span class="n">new_shape</span><span class="nf">.is_empty</span><span class="p">()</span> <span class="p">||</span> <span class="p">(</span><span class="n">new_shape</span><span class="nf">.len</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">return</span> <span class="nf">Err</span><span class="p">(</span><span class="nn">TensorError</span><span class="p">::</span><span class="n">MaxDimsError</span><span class="p">);</span>
        <span class="p">}</span>

        <span class="c">// allocate a new vector for the result of the op</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">new_data</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;</span> <span class="o">=</span>
            <span class="nn">Vec</span><span class="p">::</span><span class="nf">with_capacity</span><span class="p">(</span><span class="nn">Tensor</span><span class="p">::</span>
                <span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="n">new_shape</span><span class="p">));</span>

        <span class="c">// call `Tensor::op()` on each element in the tensor</span>
        <span class="k">for</span> <span class="n">i</span> <span class="n">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">new_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">{</span>
            <span class="k">if</span> <span class="n">new_shape</span><span class="nf">.len</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="p">{</span>
                <span class="k">let</span> <span class="n">op_result</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">op</span><span class="p">(</span>
                    <span class="o">&amp;</span><span class="k">self</span><span class="py">.data</span><span class="p">[</span><span class="nn">Tensor</span><span class="p">::</span>
                        <span class="nf">get_physical_idx</span><span class="p">(</span><span class="o">&amp;</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="o">&amp;</span><span class="n">lhs_strides</span><span class="p">)],</span>
                    <span class="o">&amp;</span><span class="n">other</span><span class="py">.data</span><span class="p">[</span><span class="nn">Tensor</span><span class="p">::</span>
                        <span class="nf">get_physical_idx</span><span class="p">(</span><span class="o">&amp;</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="o">&amp;</span><span class="n">rhs_strides</span><span class="p">)],</span>
                    <span class="o">&amp;</span><span class="n">op</span><span class="p">,</span>
                <span class="p">)</span><span class="o">?</span><span class="p">;</span>

                <span class="n">new_data</span><span class="nf">.push</span><span class="p">(</span><span class="n">op_result</span><span class="p">);</span>
            <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                <span class="k">for</span> <span class="n">j</span> <span class="n">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">new_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">{</span>
                    <span class="k">let</span> <span class="n">op_result</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">op</span><span class="p">(</span>
                        <span class="o">&amp;</span><span class="k">self</span><span class="py">.data</span><span class="p">[</span><span class="nn">Tensor</span><span class="p">::</span>
                            <span class="nf">get_physical_idx</span><span class="p">(</span><span class="o">&amp;</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="o">&amp;</span><span class="n">lhs_strides</span><span class="p">)],</span>
                        <span class="o">&amp;</span><span class="n">other</span><span class="py">.data</span><span class="p">[</span><span class="nn">Tensor</span><span class="p">::</span>
                            <span class="nf">get_physical_idx</span><span class="p">(</span><span class="o">&amp;</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="o">&amp;</span><span class="n">rhs_strides</span><span class="p">)],</span>
                        <span class="o">&amp;</span><span class="n">op</span><span class="p">,</span>
                    <span class="p">)</span><span class="o">?</span><span class="p">;</span>

                    <span class="n">new_data</span><span class="nf">.push</span><span class="p">(</span><span class="n">op_result</span><span class="p">);</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">}</span>

        <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">new_shape</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>Let’s also overload Rust’s built-in <code class="language-plaintext highlighter-rouge">Add</code>, <code class="language-plaintext highlighter-rouge">Sub</code>, <code class="language-plaintext highlighter-rouge">Mul</code>, and <code class="language-plaintext highlighter-rouge">Div</code> traits for <code class="language-plaintext highlighter-rouge">Tensor</code> so we can use the native plus and minus operators on <code class="language-plaintext highlighter-rouge">Tensor</code>s: <code class="language-plaintext highlighter-rouge">let c: Tensor = a + b;</code></p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/tensor.rs</span>

<span class="o">...</span>

<span class="k">impl</span> <span class="nb">Add</span> <span class="k">for</span> <span class="o">&amp;</span><span class="n">Tensor</span> <span class="p">{</span>
    <span class="k">type</span> <span class="n">Output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">;</span>

    <span class="k">fn</span> <span class="nf">add</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Tensor</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span><span class="nf">.tensor_op</span><span class="p">(</span><span class="n">other</span><span class="p">,</span>
            <span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Add</span><span class="p">))</span> <span class="p">{</span>
            <span class="nf">Ok</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="n">t</span><span class="p">,</span>
            <span class="nf">Err</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nd">panic!</span><span class="p">(</span><span class="s">"{}"</span><span class="p">,</span> <span class="n">e</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="nb">Sub</span> <span class="k">for</span> <span class="o">&amp;</span><span class="n">Tensor</span> <span class="p">{</span>
    <span class="k">type</span> <span class="n">Output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">;</span>

    <span class="k">fn</span> <span class="nf">sub</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Tensor</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span><span class="nf">.tensor_op</span><span class="p">(</span><span class="n">other</span><span class="p">,</span>
            <span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Sub</span><span class="p">))</span> <span class="p">{</span>
            <span class="nf">Ok</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="n">t</span><span class="p">,</span>
            <span class="nf">Err</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nd">panic!</span><span class="p">(</span><span class="s">"{}"</span><span class="p">,</span> <span class="n">e</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="nb">Mul</span> <span class="k">for</span> <span class="o">&amp;</span><span class="n">Tensor</span> <span class="p">{</span>
    <span class="k">type</span> <span class="n">Output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">;</span>

    <span class="k">fn</span> <span class="nf">mul</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Tensor</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span><span class="nf">.tensor_op</span><span class="p">(</span><span class="n">other</span><span class="p">,</span>
            <span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Mul</span><span class="p">))</span> <span class="p">{</span>
            <span class="nf">Ok</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="n">t</span><span class="p">,</span>
            <span class="nf">Err</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nd">panic!</span><span class="p">(</span><span class="s">"{}"</span><span class="p">,</span> <span class="n">e</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="nb">Div</span> <span class="k">for</span> <span class="o">&amp;</span><span class="n">Tensor</span> <span class="p">{</span>
    <span class="k">type</span> <span class="n">Output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">;</span>

    <span class="k">fn</span> <span class="nf">div</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Tensor</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span><span class="nf">.tensor_op</span><span class="p">(</span><span class="n">other</span><span class="p">,</span>
            <span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Div</span><span class="p">))</span> <span class="p">{</span>
            <span class="nf">Ok</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="n">t</span><span class="p">,</span>
            <span class="nf">Err</span><span class="p">(</span><span class="n">e</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nd">panic!</span><span class="p">(</span><span class="s">"{}"</span><span class="p">,</span> <span class="n">e</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>Now that we have the ops implemented, all we need to do now is to add <code class="language-plaintext highlighter-rouge">ops.rs</code> as a module in <code class="language-plaintext highlighter-rouge">lib.rs</code></p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/lib.rs</span>

<span class="k">mod</span> <span class="n">ops</span><span class="p">;</span>
</code></pre></div></div>

<hr />

<p>and let’s try it out:</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/bin/main.rs</span>

<span class="k">fn</span> <span class="nf">main</span><span class="p">()</span> <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="p">(),</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="k">let</span> <span class="n">a</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="o">&amp;</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">?</span><span class="p">;</span>
    <span class="k">let</span> <span class="n">b</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="o">&amp;</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span><span class="o">?</span><span class="p">;</span>

    <span class="k">let</span> <span class="n">c</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">a</span> <span class="o">*</span> <span class="o">&amp;</span><span class="n">b</span><span class="p">;</span>

    <span class="nd">println!</span><span class="p">(</span><span class="s">"{}"</span><span class="p">,</span> <span class="n">c</span><span class="p">);</span>

    <span class="o">...</span>
</code></pre></div></div>

<hr />

<p>Just run <code class="language-plaintext highlighter-rouge">cargo run</code> in your terminal to see the results:</p>

<hr />

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bilal@Bilals-MacBook-Pro l2 % cargo run
    Finished dev <span class="o">[</span>unoptimized + debuginfo] target<span class="o">(</span>s<span class="o">)</span> <span class="k">in </span>0.75s
    Running <span class="sb">`</span>target/debug/main<span class="sb">`</span>
Value: <span class="o">[</span>1.0, 4.0, 9.0]
Shape: <span class="o">[</span>1, 3]
</code></pre></div></div>

<hr />

<h1 id="autograd">Autograd</h1>

<hr />

<p>We need to implement one more operator before we can start working on our autograd system: <code class="language-plaintext highlighter-rouge">.pow()</code></p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/ops.rs</span>
<span class="nd">#[derive(Debug,</span> <span class="nd">PartialEq)]</span>
<span class="k">pub</span> <span class="k">enum</span> <span class="n">Ops</span> <span class="p">{</span>
    <span class="o">...</span>
    <span class="nf">Pow</span><span class="p">(</span><span class="nb">f32</span><span class="p">),</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Display</span> <span class="k">for</span> <span class="n">Ops</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">fmt</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Formatter</span><span class="o">&lt;</span><span class="nv">'_</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span>
        <span class="nn">fmt</span><span class="p">::</span><span class="n">Result</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span> <span class="p">{</span>
            <span class="o">...</span>
            <span class="nn">Ops</span><span class="p">::</span><span class="nf">Pow</span><span class="p">(</span><span class="n">pow</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nd">write!</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"Pow: {}"</span><span class="p">,</span> <span class="n">pow</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/tensor.rs</span>

<span class="k">impl</span> <span class="n">Tensor</span> <span class="p">{</span>

    <span class="o">...</span>

    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">pow</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">exp</span><span class="p">:</span> <span class="nb">f32</span><span class="p">)</span> <span class="k">-&gt;</span>
        <span class="n">Result</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="k">let</span> <span class="n">new_data</span> <span class="o">=</span> <span class="k">self</span><span class="py">.data</span><span class="nf">.iter</span><span class="p">()</span>
            <span class="nf">.map</span><span class="p">(|</span><span class="n">val</span><span class="p">|</span> <span class="n">val</span><span class="nf">.powf</span><span class="p">(</span><span class="n">exp</span><span class="p">))</span><span class="nf">.collect</span><span class="p">();</span>

        <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span> <span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>Now that that’s out of the way, lets go on to the fun stuff: Autograd. We’ll be implementing a simple but efficient graph based autograd system similar to what PyTorch uses.</p>

<p>Every <code class="language-plaintext highlighter-rouge">Tensor</code> struct will hold field(s) that hold references to its parent(s) as well as a field holding the op that was used to create it and a <code class="language-plaintext highlighter-rouge">Vec&lt;f32&gt;</code> to store its gradient.</p>

<p>Since we’re using Rust, a language famous for its focus on guaranteeing memory safety at compile time, we’ll need to put a little bit of thought into how to implement all this. A <code class="language-plaintext highlighter-rouge">Tensor</code> may or may not have either one or two immutable references to its parent <code class="language-plaintext highlighter-rouge">Tensors</code> and also may or may not have been created using an <code class="language-plaintext highlighter-rouge">Op</code>. We also need a way to compute a <code class="language-plaintext highlighter-rouge">Tensor</code>’s gradient wrt to its children.</p>

<p>To make everything simple, we’ll wrap the gradient of a <code class="language-plaintext highlighter-rouge">Tensor</code> in a <code class="language-plaintext highlighter-rouge">RefCell</code> so we can safely change its value by calling <code class="language-plaintext highlighter-rouge">.borrow_mut()</code> without needing to keep a mutable reference to it. <em>Keeping a mutable reference might not be possible if one <code class="language-plaintext highlighter-rouge">Tensor</code> has two distinct children — Rust only allows one mutable reference to be in scope at a time.</em></p>

<p>Let’s get started by adding a few field to our original <code class="language-plaintext highlighter-rouge">Tensor</code> struct:</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/tensor.rs</span>

<span class="k">use</span> <span class="nn">std</span><span class="p">::</span><span class="nn">cell</span><span class="p">::</span><span class="n">RefCell</span><span class="p">;</span>

<span class="k">pub</span> <span class="k">struct</span> <span class="n">Tensor</span> <span class="p">{</span>
    <span class="o">...</span>

    <span class="n">track_grad</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>

    <span class="n">lhs_parent</span><span class="p">:</span> <span class="nb">Option</span><span class="o">&lt;&amp;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">rhs_parent</span><span class="p">:</span> <span class="nb">Option</span><span class="o">&lt;&amp;</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">create_op</span><span class="p">:</span> <span class="nb">Option</span><span class="o">&lt;</span><span class="n">Ops</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">derivative</span><span class="p">:</span> <span class="n">RefCell</span><span class="o">&lt;</span><span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;&gt;</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>If you add this and press <code class="language-plaintext highlighter-rouge">⌘-S</code>, you’ll probably see that rust-analyzer starts throwing out dozens of warnings and errors. Now that we’re storing references to other <code class="language-plaintext highlighter-rouge">Tensor</code>s inside our <code class="language-plaintext highlighter-rouge">Tensor</code>, we need to add lifetime parameters to our struct so the Rust compiler can make sure that these references don’t go out of scope during any part of our program.</p>

<p>If you’re using VSCode with rust-analyzer like I am, fixing lifetime errors in Rust is pretty painless when the compiler literally guides you through it and tells you where the problem is, why it exists, and how to fix it :)</p>

<p>Here’s a diff showing the changes that I had to make:</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/tensor.rs</span>

<span class="o">-</span><span class="k">pub</span> <span class="k">struct</span> <span class="n">Tensor</span> <span class="p">{</span>
<span class="o">+</span><span class="k">pub</span> <span class="k">struct</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="o">...</span>

<span class="o">-</span>    <span class="n">lhs_parent</span><span class="p">:</span> <span class="nb">Option</span><span class="o">&lt;&amp;</span><span class="n">Tensor</span><span class="p">,</span>
<span class="o">-</span>    <span class="n">rhs_parent</span><span class="p">:</span> <span class="nb">Option</span><span class="o">&lt;&amp;</span><span class="n">Tensor</span><span class="p">,</span>
<span class="o">+</span>    <span class="n">lhs_parent</span><span class="p">:</span> <span class="nb">Option</span><span class="o">&lt;&amp;</span><span class="nv">'a</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;&gt;</span><span class="p">,</span>
<span class="o">+</span>    <span class="n">rhs_parent</span><span class="p">:</span> <span class="nb">Option</span><span class="o">&lt;&amp;</span><span class="nv">'a</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;&gt;</span><span class="p">,</span>
<span class="p">}</span>

<span class="o">-</span><span class="k">impl</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Display</span> <span class="k">for</span> <span class="n">Tensor</span> <span class="p">{</span>
<span class="o">+</span><span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Display</span> <span class="k">for</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="o">...</span>
<span class="p">}</span>

<span class="o">-</span><span class="k">impl</span> <span class="n">Clone</span> <span class="k">for</span> <span class="n">Tensor</span> <span class="p">{</span>
<span class="o">+</span><span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="n">Clone</span> <span class="k">for</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="o">...</span>
<span class="p">}</span>

<span class="o">-</span><span class="k">impl</span> <span class="n">Tensor</span> <span class="p">{</span>
<span class="o">+</span><span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="o">...</span>

<span class="o">-</span>    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">new</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">usize</span><span class="p">])</span>
<span class="o">-</span>        <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
<span class="o">+</span>    <span class="k">pub</span> <span class="k">fn</span> <span class="n">new</span><span class="o">&lt;</span><span class="nv">'b</span><span class="o">&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">shape</span><span class="p">:</span> <span class="o">&amp;</span><span class="p">[</span><span class="nb">usize</span><span class="p">])</span>
<span class="o">+</span>        <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'b</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="k">if</span> <span class="n">data</span><span class="nf">.len</span><span class="p">()</span> <span class="o">==</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
            <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">shape</span><span class="nf">.is_empty</span><span class="p">()</span>
            <span class="o">&amp;&amp;</span> <span class="n">shape</span><span class="nf">.len</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">3</span>
        <span class="p">{</span>
            <span class="nf">Ok</span><span class="p">(</span><span class="n">Tensor</span> <span class="p">{</span>
                <span class="n">data</span><span class="p">,</span>
                <span class="n">shape</span><span class="p">:</span> <span class="n">shape</span><span class="nf">.to_vec</span><span class="p">(),</span>
                <span class="n">strides</span><span class="p">:</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_strides_from_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span>
<span class="o">+</span>                <span class="n">track_grad</span><span class="p">:</span> <span class="k">true</span><span class="p">,</span>
<span class="o">+</span>                <span class="n">create_op</span><span class="p">:</span> <span class="nb">None</span><span class="p">,</span>
<span class="o">+</span>                <span class="n">derivative</span><span class="p">:</span> <span class="nn">RefCell</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span>
<span class="o">+</span>                    <span class="nd">vec!</span><span class="p">[</span><span class="mf">0.0</span><span class="p">;</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="n">shape</span><span class="p">)]),</span>
<span class="o">+</span>                <span class="n">lhs_parent</span><span class="p">:</span> <span class="nb">None</span><span class="p">,</span>
<span class="o">+</span>                <span class="n">rhs_parent</span><span class="p">:</span> <span class="nb">None</span><span class="p">,</span>
            <span class="p">})</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="nf">Err</span><span class="p">(</span><span class="nn">TensorError</span><span class="p">::</span><span class="n">InvalidTensor</span><span class="p">)</span>
        <span class="p">}</span>
    <span class="p">}</span>

<span class="p">}</span>

<span class="o">-</span><span class="k">impl</span> <span class="nb">Add</span> <span class="k">for</span> <span class="o">&amp;</span><span class="n">Tensor</span> <span class="p">{</span>
<span class="o">-</span>    <span class="k">type</span> <span class="n">Output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">;</span>
<span class="o">+</span><span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="nb">Add</span> <span class="k">for</span> <span class="o">&amp;</span><span class="nv">'a</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
<span class="o">+</span>    <span class="k">type</span> <span class="n">Output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span><span class="p">;</span>

<span class="o">-</span>    <span class="k">fn</span> <span class="nf">add</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Tensor</span> <span class="p">{</span>
<span class="o">+</span>    <span class="k">fn</span> <span class="nf">add</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="nv">'a</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="o">...</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="o">-</span><span class="k">impl</span> <span class="nb">Sub</span> <span class="k">for</span> <span class="o">&amp;</span><span class="n">Tensor</span> <span class="p">{</span>
<span class="o">-</span>    <span class="k">type</span> <span class="n">Output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">;</span>
<span class="o">+</span><span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="nb">Sub</span> <span class="k">for</span> <span class="o">&amp;</span><span class="nv">'a</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
<span class="o">+</span>    <span class="k">type</span> <span class="n">Output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span><span class="p">;</span>

<span class="o">-</span>    <span class="k">fn</span> <span class="nf">sub</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Tensor</span> <span class="p">{</span>
<span class="o">+</span>    <span class="k">fn</span> <span class="nf">sub</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="nv">'a</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="o">...</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="o">-</span><span class="k">impl</span> <span class="nb">Mul</span> <span class="k">for</span> <span class="o">&amp;</span><span class="n">Tensor</span> <span class="p">{</span>
<span class="o">-</span>    <span class="k">type</span> <span class="n">Output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">;</span>
<span class="o">+</span><span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="nb">Mul</span> <span class="k">for</span> <span class="o">&amp;</span><span class="nv">'a</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
<span class="o">+</span>    <span class="k">type</span> <span class="n">Output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span><span class="p">;</span>

<span class="o">-</span>    <span class="k">fn</span> <span class="nf">mul</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Tensor</span> <span class="p">{</span>
<span class="o">+</span>    <span class="k">fn</span> <span class="nf">mul</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="nv">'a</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="o">...</span>
    <span class="p">}</span>
<span class="p">}</span>

<span class="o">-</span><span class="k">impl</span> <span class="nb">Div</span> <span class="k">for</span> <span class="o">&amp;</span><span class="n">Tensor</span> <span class="p">{</span>
<span class="o">-</span>    <span class="k">type</span> <span class="n">Output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="p">;</span>
<span class="o">+</span><span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="nb">Div</span> <span class="k">for</span> <span class="o">&amp;</span><span class="nv">'a</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
<span class="o">+</span>    <span class="k">type</span> <span class="n">Output</span> <span class="o">=</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span><span class="p">;</span>

<span class="o">-</span>    <span class="k">fn</span> <span class="nf">div</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Tensor</span> <span class="p">{</span>
<span class="o">+</span>    <span class="k">fn</span> <span class="nf">div</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="nv">'a</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="o">...</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p><em>Note: you might notice that you don’t need to declare a lifetime parameter on <code class="language-plaintext highlighter-rouge">other</code> in the <code class="language-plaintext highlighter-rouge">impl</code> blocks for <code class="language-plaintext highlighter-rouge">Add</code>, <code class="language-plaintext highlighter-rouge">Sub</code>, <code class="language-plaintext highlighter-rouge">Mul</code>, and <code class="language-plaintext highlighter-rouge">Div</code>. I’m including the lifetime parameters here since we’ll need to add them in the next step since the output of <code class="language-plaintext highlighter-rouge">Tensor::tensorop()</code> will store a reference to <code class="language-plaintext highlighter-rouge">other</code> as one of its parents. This means that lifetime parameters will be needed to make sure that the reference to the parent remains valid for the full lifetime of the output.</em></p>

<p>Now that we’ve satisfied the Rust compiler, let’s modify <code class="language-plaintext highlighter-rouge">Tensor::tensor_op()</code> and <code class="language-plaintext highlighter-rouge">Tensor::pow()</code> to use the new struct fields we just added.</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/tensor.rs</span>

<span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="o">...</span>

<span class="o">-</span>    <span class="k">fn</span> <span class="nf">tensor_op</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">Ops</span><span class="p">)</span>
<span class="o">-</span>       <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
<span class="o">+</span>    <span class="k">fn</span> <span class="nf">tensor_op</span><span class="p">(</span><span class="o">&amp;</span><span class="nv">'a</span> <span class="k">self</span><span class="p">,</span> <span class="n">other</span><span class="p">:</span> <span class="o">&amp;</span><span class="nv">'a</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">op</span><span class="p">:</span> <span class="n">Ops</span><span class="p">)</span>
<span class="o">+</span>        <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>

        <span class="o">...</span>

<span class="o">-</span>       <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">new_shape</span><span class="p">)</span>
<span class="o">+</span>        <span class="nf">Ok</span><span class="p">(</span><span class="n">Tensor</span> <span class="p">{</span>
<span class="o">+</span>            <span class="n">data</span><span class="p">:</span> <span class="n">new_data</span><span class="p">,</span>
<span class="o">+</span>            <span class="n">shape</span><span class="p">:</span> <span class="n">new_shape</span><span class="nf">.to_vec</span><span class="p">(),</span>
<span class="o">+</span>            <span class="n">strides</span><span class="p">:</span> <span class="nn">Tensor</span><span class="p">::</span>
<span class="o">+</span>                <span class="nf">calc_strides_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="n">new_shape</span><span class="p">),</span>
<span class="o">+</span>            <span class="n">track_grad</span><span class="p">:</span> <span class="k">true</span><span class="p">,</span>
<span class="o">+</span>            <span class="n">create_op</span><span class="p">:</span> <span class="nf">Some</span><span class="p">(</span><span class="n">op</span><span class="p">),</span>
<span class="o">+</span>            <span class="n">derivative</span><span class="p">:</span> <span class="nn">RefCell</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span>
<span class="o">+</span>                <span class="nd">vec!</span><span class="p">[</span><span class="mf">0.0</span><span class="p">;</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="n">new_shape</span><span class="p">)]),</span>
<span class="o">+</span>            <span class="n">lhs_parent</span><span class="p">:</span> <span class="nf">Some</span><span class="p">(</span><span class="k">self</span><span class="p">),</span>
<span class="o">+</span>            <span class="n">rhs_parent</span><span class="p">:</span> <span class="nf">Some</span><span class="p">(</span><span class="n">other</span><span class="p">),</span>
<span class="o">+</span>        <span class="p">})</span>

    <span class="p">}</span>

    <span class="o">...</span>

    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">pow</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">exp</span><span class="p">:</span> <span class="nb">f32</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>

        <span class="o">...</span>

<span class="o">-</span>       <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">new_data</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">new_shape</span><span class="p">)</span>
<span class="o">+</span>        <span class="nf">Ok</span><span class="p">(</span><span class="n">Tensor</span> <span class="p">{</span>
<span class="o">+</span>            <span class="n">data</span><span class="p">:</span> <span class="n">new_data</span><span class="p">,</span>
<span class="o">+</span>            <span class="n">shape</span><span class="p">:</span> <span class="k">self</span><span class="py">.shape</span><span class="nf">.to_vec</span><span class="p">(),</span>
<span class="o">+</span>            <span class="n">strides</span><span class="p">:</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_strides_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">),</span>
<span class="o">+</span>            <span class="n">track_grad</span><span class="p">:</span> <span class="k">true</span><span class="p">,</span>
<span class="o">+</span>            <span class="n">create_op</span><span class="p">:</span> <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="nf">Pow</span><span class="p">(</span><span class="n">exp</span><span class="p">)),</span>
<span class="o">+</span>            <span class="n">derivative</span><span class="p">:</span> <span class="nn">RefCell</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span>
<span class="o">+</span>                <span class="nd">vec!</span><span class="p">[</span><span class="mf">0.0</span><span class="p">;</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">)]),</span>
<span class="o">+</span>            <span class="n">lhs_parent</span><span class="p">:</span> <span class="nf">Some</span><span class="p">(</span><span class="k">self</span><span class="p">),</span>
<span class="o">+</span>            <span class="n">rhs_parent</span><span class="p">:</span> <span class="nb">None</span><span class="p">,</span>
<span class="o">+</span>        <span class="p">})</span>
    <span class="p">}</span>
<span class="p">}</span>

</code></pre></div></div>

<hr />

<p>Ok, we’re halfway there! We can now represent a sequence of operations as a computation graph. Let’s update our pretty-printing code to print out the structure of our internal representation (IR) of the computation graph.</p>

<p>This probably isn’t the most elegant way of implementing this but it works and I’m not motivated enough right now to try and improve it.</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/tensor.rs</span>

<span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Display</span> <span class="k">for</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">fmt</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Formatter</span><span class="o">&lt;</span><span class="nv">'_</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Result</span> <span class="p">{</span>
        <span class="k">fn</span> <span class="nf">recurse</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">level</span><span class="p">:</span> <span class="nb">usize</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nb">String</span> <span class="p">{</span>
            <span class="k">let</span> <span class="n">indent</span> <span class="o">=</span> <span class="s">"  "</span><span class="nf">.to_string</span><span class="p">()</span><span class="nf">.repeat</span><span class="p">(</span><span class="n">level</span><span class="p">);</span>

            <span class="k">let</span> <span class="n">lhs</span> <span class="o">=</span> <span class="k">match</span> <span class="n">tensor</span><span class="py">.lhs_parent</span> <span class="p">{</span>
                <span class="nf">Some</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nf">recurse</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">level</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
                <span class="nb">None</span> <span class="k">=&gt;</span> <span class="s">"None"</span><span class="nf">.to_string</span><span class="p">(),</span>
            <span class="p">};</span>

            <span class="k">let</span> <span class="n">rhs</span> <span class="o">=</span> <span class="k">match</span> <span class="n">tensor</span><span class="py">.rhs_parent</span> <span class="p">{</span>
                <span class="nf">Some</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nf">recurse</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">level</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span>
                <span class="nb">None</span> <span class="k">=&gt;</span> <span class="s">"None"</span><span class="nf">.to_string</span><span class="p">(),</span>
            <span class="p">};</span>

            <span class="k">let</span> <span class="n">op</span> <span class="o">=</span> <span class="k">match</span> <span class="o">&amp;</span><span class="n">tensor</span><span class="py">.create_op</span> <span class="p">{</span>
                <span class="nf">Some</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nd">format!</span><span class="p">(</span><span class="s">"{}"</span><span class="p">,</span> <span class="n">t</span><span class="p">),</span>
                <span class="nb">None</span> <span class="k">=&gt;</span> <span class="s">"None"</span><span class="nf">.to_string</span><span class="p">(),</span>
            <span class="p">};</span>

            <span class="nd">format!</span><span class="p">(</span>
                <span class="s">"</span><span class="se">\n</span><span class="s">{}Value: {:?} </span><span class="se">\n</span><span class="s">{}Shape: {:?} </span><span class="se">\n</span><span class="s">{}Lhs: {} </span><span class="se">\n</span><span class="s">{}Rhs: {} </span><span class="se">\n</span><span class="s">{}Op: {} </span><span class="se">\n</span><span class="s">{}TrackGrad: {:?} </span><span class="se">\n</span><span class="s">{}Derivative: {:?}"</span><span class="p">,</span>
                <span class="n">indent</span><span class="p">,</span>
                <span class="n">tensor</span><span class="py">.data</span><span class="p">,</span>
                <span class="n">indent</span><span class="p">,</span>
                <span class="n">tensor</span><span class="py">.shape</span><span class="p">,</span>
                <span class="n">indent</span><span class="p">,</span>
                <span class="n">lhs</span><span class="p">,</span>
                <span class="n">indent</span><span class="p">,</span>
                <span class="n">rhs</span><span class="p">,</span>
                <span class="n">indent</span><span class="p">,</span>
                <span class="n">op</span><span class="p">,</span>
                <span class="n">indent</span><span class="p">,</span>
                <span class="n">tensor</span><span class="py">.track_grad</span><span class="p">,</span>
                <span class="n">indent</span><span class="p">,</span>
                <span class="o">*</span><span class="p">(</span><span class="n">tensor</span><span class="py">.derivative</span><span class="nf">.borrow</span><span class="p">())</span>
            <span class="p">)</span>
        <span class="p">}</span>

        <span class="k">let</span> <span class="n">graph</span> <span class="o">=</span> <span class="nf">recurse</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

        <span class="nd">write!</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"{}"</span><span class="p">,</span> <span class="n">graph</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/bin/main.rs</span>

<span class="k">fn</span> <span class="nf">main</span><span class="p">()</span> <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="p">(),</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="k">let</span> <span class="n">a</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="o">&amp;</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="o">?</span><span class="p">;</span>
    <span class="k">let</span> <span class="n">b</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="o">&amp;</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span><span class="o">?</span><span class="p">;</span>

    <span class="k">let</span> <span class="n">c</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">a</span> <span class="o">*</span> <span class="o">&amp;</span><span class="n">b</span><span class="p">;</span>

    <span class="k">let</span> <span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="nf">.pow</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>

    <span class="nd">println!</span><span class="p">(</span><span class="s">"{}"</span><span class="p">,</span> <span class="n">d</span><span class="p">);</span>

    <span class="nf">Ok</span><span class="p">(())</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>Let’s run this and take a look at the resulting IR:</p>

<hr />

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bilal@Bilals-MacBook-Pro l2 % cargo run
Value: <span class="o">[</span>1.0, 16.0, 81.0]
Shape: <span class="o">[</span>1, 3]
Lhs:
  Value: <span class="o">[</span>1.0, 4.0, 9.0]
  Shape: <span class="o">[</span>1, 3]
  Lhs:
    Value: <span class="o">[</span>1.0, 2.0, 3.0]
    Shape: <span class="o">[</span>1, 3]
    Lhs: None
    Rhs: None
    Op: None
    TrackGrad: <span class="nb">true
    </span>Derivative: <span class="o">[</span>0.0, 0.0, 0.0]
  Rhs:
    Value: <span class="o">[</span>1.0, 2.0, 3.0]
    Shape: <span class="o">[</span>3]
    Lhs: None
    Rhs: None
    Op: None
    TrackGrad: <span class="nb">true
    </span>Derivative: <span class="o">[</span>0.0, 0.0, 0.0]
  Op: Multiply
  TrackGrad: <span class="nb">true
  </span>Derivative: <span class="o">[</span>0.0, 0.0, 0.0]
Rhs: None
Op: Pow: 2
TrackGrad: <span class="nb">true
</span>Derivative: <span class="o">[</span>0.0, 0.0, 0.0]
</code></pre></div></div>

<hr />

<p>Maybe it’s not the nicest looking graph, but it works well for when you’re trying to visually verify that your gradients are being calculated correctly.</p>

<hr />

<p>Now that we have a computation graph, we need to find a way to backpropogate through it.</p>

<p>The simplest way would be to recursively call a function named <code class="language-plaintext highlighter-rouge">backward()</code> on the tensor you want to calculate the gradient with respect to. <code class="language-plaintext highlighter-rouge">backward()</code> would first take the gradient of the current tensor (the gradient of the output tensor would be with respect to itself so its gradient is $1$) then use it to calculate (and accumulate, if necessary) the gradient of its parent(s) before calling <code class="language-plaintext highlighter-rouge">.backward()</code> on the parent <code class="language-plaintext highlighter-rouge">Tensor</code>(s) to recursively calculate the gradient for the entire computation graph.</p>

<p>There are a couple of problems with this:</p>

<p>First, recursively calling <code class="language-plaintext highlighter-rouge">.backward()</code> on the entire computation graph would be very memory-inefficient.</p>

<p>Second, if the computation graph has multiple branches (like in a Resnet), the backwards pass over the computation graph will have to be computed multiple times as the gradients for the parent <code class="language-plaintext highlighter-rouge">Tensor</code> of each branch in the network are accumulated. Doing it this way would have make computing the backwards pass <em>very</em> slow and inefficient.</p>

<p>Luckily, there is a better way of doing this. If we topologically sort and reverse the graph so that all the <code class="language-plaintext highlighter-rouge">Tensor</code>s are ordered in a way so that the gradients for all child <code class="language-plaintext highlighter-rouge">Tensor</code>s of a certain <code class="language-plaintext highlighter-rouge">Tensor</code> have already been computed and the gradient for the current <code class="language-plaintext highlighter-rouge">Tensor</code> has already been accumulated (if necessary), we won’t have to re-compute any parts of the graph.</p>

<p>Let’s see how we could implement this in Rust:</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/tensor.rs</span>

<span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>

    <span class="o">...</span>

    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">backward</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">)</span> <span class="p">{</span>
        <span class="c">// from https://github.com/evcu/numpy_autograd/blob/master/my_autograd.py#L147</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">seen</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;&amp;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="o">=</span> <span class="nn">Vec</span><span class="p">::</span><span class="nf">new</span><span class="p">();</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">sorted</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;&amp;</span><span class="n">Tensor</span><span class="o">&gt;</span> <span class="o">=</span> <span class="nn">Vec</span><span class="p">::</span><span class="nf">new</span><span class="p">();</span>

        <span class="k">fn</span> <span class="n">topological_sort</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span><span class="p">(</span>
            <span class="n">vr</span><span class="p">:</span> <span class="o">&amp;</span><span class="nv">'a</span> <span class="n">Tensor</span><span class="p">,</span>
            <span class="n">seen</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nb">Vec</span><span class="o">&lt;&amp;</span><span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;&gt;</span><span class="p">,</span>
            <span class="n">sorted</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nb">Vec</span><span class="o">&lt;&amp;</span><span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;&gt;</span><span class="p">,</span>
        <span class="p">)</span> <span class="p">{</span>
            <span class="k">if</span> <span class="o">!</span><span class="n">seen</span><span class="nf">.contains</span><span class="p">(</span><span class="o">&amp;</span><span class="n">vr</span><span class="p">)</span> <span class="o">&amp;&amp;</span> <span class="p">(</span><span class="n">vr</span><span class="py">.lhs_parent</span><span class="nf">.is_some</span><span class="p">()</span>
                    <span class="p">||</span> <span class="n">vr</span><span class="py">.rhs_parent</span><span class="nf">.is_some</span><span class="p">())</span> <span class="p">{</span>
                <span class="n">seen</span><span class="nf">.push</span><span class="p">(</span><span class="n">vr</span><span class="p">);</span>

                <span class="k">if</span> <span class="n">vr</span><span class="py">.lhs_parent</span><span class="nf">.is_some</span><span class="p">()</span> <span class="p">{</span>
                    <span class="nf">topological_sort</span><span class="p">(</span><span class="n">vr</span><span class="py">.lhs_parent</span><span class="nf">.unwrap</span><span class="p">(),</span>
                        <span class="n">seen</span><span class="p">,</span> <span class="n">sorted</span><span class="p">);</span>
                <span class="p">}</span>
                <span class="k">if</span> <span class="n">vr</span><span class="py">.rhs_parent</span><span class="nf">.is_some</span><span class="p">()</span> <span class="p">{</span>
                    <span class="nf">topological_sort</span><span class="p">(</span><span class="n">vr</span><span class="py">.rhs_parent</span><span class="nf">.unwrap</span><span class="p">(),</span>
                        <span class="n">seen</span><span class="p">,</span> <span class="n">sorted</span><span class="p">);</span>
                <span class="p">}</span>

                <span class="n">sorted</span><span class="nf">.push</span><span class="p">(</span><span class="n">vr</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">}</span>

        <span class="c">// Topologically sort the computation graph</span>
        <span class="nf">topological_sort</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="n">seen</span><span class="p">,</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="n">sorted</span><span class="p">);</span>

        <span class="c">// reverse it</span>
        <span class="n">sorted</span><span class="nf">.reverse</span><span class="p">();</span>

        <span class="c">// Set the derivative of the output of the computation</span>
        <span class="c">// graph to itself to equal 1 (usually the derivative</span>
        <span class="c">// of the loss wrt itself)</span>
        <span class="o">*</span><span class="n">sorted</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="py">.derivative</span><span class="nf">.borrow_mut</span><span class="p">()</span> <span class="o">=</span> <span class="nd">vec!</span><span class="p">[</span><span class="mf">1.0</span><span class="p">;</span>
            <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="n">sorted</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="py">.shape</span><span class="p">)];</span>

        <span class="k">for</span> <span class="n">t</span> <span class="n">in</span> <span class="n">sorted</span><span class="nf">.iter</span><span class="p">()</span> <span class="p">{</span>
            <span class="n">t</span><span class="nf">.grad</span><span class="p">()</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>The <code class="language-plaintext highlighter-rouge">.grad()</code> function doens’t exist yet, but its purpose is to take the gradient of the current <code class="language-plaintext highlighter-rouge">Tensor</code> <code class="language-plaintext highlighter-rouge">t</code> and use it to compute the gradients of its parent(s). Since we wrapped the <code class="language-plaintext highlighter-rouge">derivative</code> field of <code class="language-plaintext highlighter-rouge">Tensor</code> in a <code class="language-plaintext highlighter-rouge">RefCell()</code>, we can use something like <code class="language-plaintext highlighter-rouge">*lhs_parent.borrow_mut() = gradient;</code> to safely mutate the parent’s gradient.</p>

<p>Here’s how I did it:</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/tensor.rs</span>

<span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">grad</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">)</span> <span class="p">{</span>
        <span class="c">// get the gradient of the derivative of self wrt output</span>
        <span class="c">// d_x/d_loss</span>
        <span class="k">let</span> <span class="n">d</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="k">self</span><span class="py">.derivative</span><span class="nf">.borrow</span><span class="p">()</span><span class="nf">.clone</span><span class="p">(),</span>
            <span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">();</span>

        <span class="c">// if lhs_parent exists</span>
        <span class="k">if</span> <span class="k">let</span> <span class="nf">Some</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">=</span> <span class="k">self</span><span class="py">.lhs_parent</span> <span class="p">{</span>

            <span class="c">// calculate the gradient of lhs_parent wrt x</span>
            <span class="c">// d_lhs/d_x</span>
            <span class="k">let</span> <span class="n">d_lhs</span> <span class="o">=</span> <span class="k">match</span> <span class="o">&amp;</span><span class="k">self</span><span class="py">.create_op</span> <span class="p">{</span>
                <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Add</span><span class="p">))</span> <span class="k">=&gt;</span>
                    <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mf">1.0</span><span class="p">;</span>
                        <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">)],</span>
                        <span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Sub</span><span class="p">))</span> <span class="k">=&gt;</span>
                    <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span>
                        <span class="nd">vec!</span><span class="p">[</span><span class="mf">1.0</span><span class="p">;</span>
                        <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">)],</span>
                        <span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Mul</span><span class="p">))</span> <span class="k">=&gt;</span>
                    <span class="nf">Ok</span><span class="p">(</span><span class="k">self</span><span class="py">.rhs_parent</span><span class="nf">.unwrap</span><span class="p">()</span><span class="nf">.clone</span><span class="p">()),</span>
                <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Div</span><span class="p">))</span> <span class="k">=&gt;</span> <span class="p">{</span>
                    <span class="k">let</span> <span class="n">temp</span> <span class="o">=</span> <span class="k">self</span><span class="py">.rhs_parent</span><span class="nf">.unwrap</span><span class="p">()</span>
                        <span class="nf">.pow</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">();</span>

                    <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">temp</span><span class="py">.data</span><span class="nf">.clone</span><span class="p">(),</span> <span class="o">&amp;</span><span class="n">temp</span><span class="py">.shape</span><span class="p">)</span>
                <span class="p">}</span>
                <span class="mi">_</span> <span class="k">=&gt;</span> <span class="nf">Err</span><span class="p">(</span><span class="nn">TensorError</span><span class="p">::</span><span class="n">GradError</span><span class="p">),</span>
            <span class="p">}</span>
            <span class="nf">.unwrap</span><span class="p">();</span>

            <span class="c">// calculate the gradient of lhs_parent wrt loss</span>
            <span class="c">// d_lhs/d_loss = d_lhs/d_x * d_x/d_loss</span>
            <span class="k">let</span> <span class="n">d_lhs</span> <span class="o">=</span> <span class="k">match</span> <span class="k">self</span><span class="py">.create_op</span> <span class="p">{</span>
                <span class="mi">_</span> <span class="k">=&gt;</span> <span class="o">&amp;</span><span class="n">d_lhs</span> <span class="o">*</span> <span class="o">&amp;</span><span class="n">d</span><span class="p">,</span>
            <span class="p">};</span>

            <span class="c">// accumulate the gradient of d_lhs/d_loss if necessary</span>
            <span class="k">let</span> <span class="n">d_lhs_prev</span> <span class="o">=</span>
                <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">t</span><span class="py">.derivative</span><span class="nf">.borrow</span><span class="p">()</span><span class="nf">.clone</span><span class="p">(),</span> <span class="o">&amp;</span><span class="n">t</span><span class="py">.shape</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">();</span>
            <span class="k">let</span> <span class="n">d_lhs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">d_lhs</span> <span class="o">+</span> <span class="o">&amp;</span><span class="n">d_lhs_prev</span><span class="p">;</span>

            <span class="c">// assign to the derivative of the parent</span>
            <span class="o">*</span><span class="n">t</span><span class="py">.derivative</span><span class="nf">.borrow_mut</span><span class="p">()</span> <span class="o">=</span> <span class="n">d_lhs</span><span class="py">.data</span><span class="p">;</span>
        <span class="p">}</span>

        <span class="c">// if rhs_parent exists</span>
        <span class="k">if</span> <span class="k">let</span> <span class="nf">Some</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">=</span> <span class="k">self</span><span class="py">.rhs_parent</span> <span class="p">{</span>

            <span class="c">// calculate the gradient of rhs_parent wrt x</span>
            <span class="c">// d_rhs/d_x</span>
            <span class="k">let</span> <span class="n">d_rhs</span> <span class="o">=</span> <span class="k">match</span> <span class="k">self</span><span class="py">.create_op</span> <span class="p">{</span>
                <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Add</span><span class="p">))</span> <span class="k">=&gt;</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span>
                    <span class="nd">vec!</span><span class="p">[</span><span class="mf">1.0</span><span class="p">;</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">)],</span>
                    <span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Sub</span><span class="p">))</span> <span class="k">=&gt;</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span>
                    <span class="nd">vec!</span><span class="p">[</span><span class="mf">1.0</span><span class="p">;</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">)],</span>
                    <span class="o">&amp;</span><span class="k">self</span><span class="py">.shape</span><span class="p">,</span>
                <span class="p">),</span>
                <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Mul</span><span class="p">))</span> <span class="k">=&gt;</span>
                    <span class="nf">Ok</span><span class="p">(</span><span class="k">self</span><span class="py">.lhs_parent</span><span class="nf">.unwrap</span><span class="p">()</span><span class="nf">.clone</span><span class="p">()),</span>
                <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="nf">TensorOp</span><span class="p">(</span><span class="nn">TensorOp</span><span class="p">::</span><span class="nb">Div</span><span class="p">))</span> <span class="k">=&gt;</span> <span class="p">{</span>
                    <span class="k">let</span> <span class="n">neg1</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="o">-</span><span class="mf">1.0</span><span class="p">],</span> <span class="o">&amp;</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="nf">.unwrap</span><span class="p">();</span>
                    <span class="k">let</span> <span class="n">t_powed</span> <span class="o">=</span> <span class="n">t</span><span class="nf">.pow</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">();</span>

                    <span class="k">let</span> <span class="n">temp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">neg1</span> <span class="o">*</span> <span class="k">self</span><span class="py">.lhs_parent</span><span class="nf">.unwrap</span><span class="p">();</span>
                    <span class="k">let</span> <span class="n">temp</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">temp</span> <span class="o">*</span> <span class="o">&amp;</span><span class="n">t_powed</span><span class="p">;</span>

                    <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">temp</span><span class="py">.data</span><span class="nf">.clone</span><span class="p">(),</span> <span class="o">&amp;</span><span class="n">temp</span><span class="py">.shape</span><span class="p">)</span>
                <span class="p">}</span>
                <span class="mi">_</span> <span class="k">=&gt;</span> <span class="nf">Err</span><span class="p">(</span><span class="nn">TensorError</span><span class="p">::</span><span class="n">GradError</span><span class="p">),</span>
            <span class="p">}</span>
            <span class="nf">.unwrap</span><span class="p">();</span>

            <span class="c">// calculate the gradient of rhs_parent wrt loss</span>
            <span class="c">// d_rhs/d_loss = d_rhs/d_x * d_x/d_loss</span>
            <span class="k">let</span> <span class="n">d_rhs</span> <span class="o">=</span> <span class="k">match</span> <span class="k">self</span><span class="py">.create_op</span> <span class="p">{</span>
                <span class="mi">_</span> <span class="k">=&gt;</span> <span class="o">&amp;</span><span class="n">d_rhs</span> <span class="o">*</span> <span class="o">&amp;</span><span class="n">d</span><span class="p">,</span>
            <span class="p">};</span>

            <span class="c">// accumulate the gradient of d_rhs/d_loss if necessary</span>
            <span class="k">let</span> <span class="n">d_rhs_prev</span> <span class="o">=</span>
                <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">t</span><span class="py">.derivative</span><span class="nf">.borrow</span><span class="p">()</span><span class="nf">.clone</span><span class="p">(),</span> <span class="o">&amp;</span><span class="n">t</span><span class="py">.shape</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">();</span>
            <span class="k">let</span> <span class="n">d_rhs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">d_rhs</span> <span class="o">+</span> <span class="o">&amp;</span><span class="n">d_rhs_prev</span><span class="p">;</span>

            <span class="c">// assign to the derivative of the parent</span>
            <span class="o">*</span><span class="n">t</span><span class="py">.derivative</span><span class="nf">.borrow_mut</span><span class="p">()</span> <span class="o">=</span> <span class="n">d_rhs</span><span class="py">.data</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>That should be pretty much it. Try it out:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/bin/main.rs</span>

<span class="k">fn</span> <span class="nf">main</span><span class="p">()</span> <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="p">(),</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="k">let</span> <span class="n">a</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mf">2.0</span><span class="p">],</span> <span class="o">&amp;</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="nf">.unwrap</span><span class="p">();</span>
    <span class="k">let</span> <span class="n">b</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mf">3.0</span><span class="p">],</span> <span class="o">&amp;</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="nf">.unwrap</span><span class="p">();</span>

    <span class="k">let</span> <span class="n">c</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">a</span> <span class="o">*</span> <span class="o">&amp;</span><span class="n">b</span><span class="p">;</span>

    <span class="n">c</span><span class="nf">.backward</span><span class="p">();</span>

    <span class="nd">println!</span><span class="p">(</span><span class="s">"{}"</span><span class="p">,</span> <span class="n">c</span><span class="p">);</span>

    <span class="nf">Ok</span><span class="p">(())</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bilal@Bilals-MacBook-Pro l2 % cargo run
Value: <span class="o">[</span>6.0]
Shape: <span class="o">[</span>1]
Lhs:
  Value: <span class="o">[</span>2.0]
  Shape: <span class="o">[</span>1]
  Lhs: None
  Rhs: None
  Op: None
  TrackGrad: <span class="nb">true
  </span>Derivative: <span class="o">[</span>3.0]
Rhs:
  Value: <span class="o">[</span>3.0]
  Shape: <span class="o">[</span>1]
  Lhs: None
  Rhs: None
  Op: None
  TrackGrad: <span class="nb">true
  </span>Derivative: <span class="o">[</span>2.0]
Op: Multiply
TrackGrad: <span class="nb">true
</span>Derivative: <span class="o">[</span>1.0]
</code></pre></div></div>

<p>🎉, you now have a semi-complete autograd engine!</p>

<hr />

<h1 id="advanced-ops">Advanced Ops</h1>

<hr />

<p>Let’s add support for fast matrix multiplications with BLAS.</p>

<p><em>todo</em> talk about blas</p>

<p>First up, lets implement the <code class="language-plaintext highlighter-rouge">transpose()</code> operator</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/ops.rs</span>

<span class="nd">#[derive(Debug,</span> <span class="nd">PartialEq)]</span>
<span class="k">pub</span> <span class="k">enum</span> <span class="n">Ops</span> <span class="p">{</span>
    <span class="o">...</span>
    <span class="n">Transpose</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Display</span> <span class="k">for</span> <span class="n">Ops</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">fmt</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Formatter</span><span class="o">&lt;</span><span class="nv">'_</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Result</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span> <span class="p">{</span>
            <span class="o">...</span>
            <span class="nn">Ops</span><span class="p">::</span><span class="n">Transpose</span> <span class="k">=&gt;</span> <span class="nd">write!</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"Transpose"</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/tensor.rs</span>

<span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">grad</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">if</span> <span class="k">let</span> <span class="nf">Some</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">=</span> <span class="k">self</span><span class="py">.lhs_parent</span> <span class="p">{</span>
            <span class="o">...</span>
            <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="n">Transpose</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mf">1.0</span><span class="p">],</span> <span class="o">&amp;</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="c">// dummy value</span>
        <span class="p">}</span>
        <span class="nf">.unwrap</span><span class="p">();</span>

        <span class="k">let</span> <span class="n">d_lhs</span> <span class="o">=</span> <span class="k">match</span> <span class="k">self</span><span class="py">.create_op</span> <span class="p">{</span>
            <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="n">Transpose</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="n">d</span><span class="nf">.transpose</span><span class="p">()</span><span class="nf">.unwrap</span><span class="p">(),</span>
            <span class="mi">_</span> <span class="k">=&gt;</span> <span class="o">&amp;</span><span class="n">d_lhs</span> <span class="o">*</span> <span class="o">&amp;</span><span class="n">d</span><span class="p">,</span>
        <span class="p">};</span>
    <span class="p">}</span>

    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">transpose</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">transposed_shape</span> <span class="o">=</span> <span class="k">self</span><span class="py">.shape</span><span class="nf">.clone</span><span class="p">();</span>
        <span class="k">let</span> <span class="k">mut</span> <span class="n">transposed_strides</span> <span class="o">=</span> <span class="k">self</span><span class="py">.strides</span><span class="nf">.clone</span><span class="p">();</span>

        <span class="n">transposed_shape</span><span class="nf">.reverse</span><span class="p">();</span>
        <span class="n">transposed_strides</span><span class="nf">.reverse</span><span class="p">();</span>

        <span class="k">let</span> <span class="k">mut</span> <span class="n">new_data</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;</span> <span class="o">=</span>
            <span class="nn">Vec</span><span class="p">::</span><span class="nf">with_capacity</span><span class="p">(</span>
                <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="n">transposed_shape</span><span class="p">));</span>

        <span class="k">for</span> <span class="n">i</span> <span class="n">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">transposed_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="p">{</span>
            <span class="k">if</span> <span class="n">transposed_shape</span><span class="nf">.len</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="p">{</span>
                <span class="n">new_data</span><span class="nf">.push</span><span class="p">(</span><span class="k">self</span><span class="py">.data</span><span class="p">[</span><span class="nn">Tensor</span><span class="p">::</span>
                    <span class="nf">get_physical_idx</span><span class="p">(</span><span class="o">&amp;</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="o">&amp;</span><span class="n">transposed_strides</span><span class="p">)]</span>
                <span class="p">);</span>
            <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
                <span class="k">for</span> <span class="n">j</span> <span class="n">in</span> <span class="mi">0</span><span class="o">..</span><span class="n">transposed_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">{</span>
                    <span class="n">new_data</span><span class="nf">.push</span><span class="p">(</span><span class="k">self</span><span class="py">.data</span><span class="p">[</span><span class="nn">Tensor</span><span class="p">::</span>
                        <span class="nf">get_physical_idx</span><span class="p">(</span><span class="o">&amp;</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="o">&amp;</span><span class="n">transposed_strides</span><span class="p">)]);</span>
                <span class="p">}</span>
            <span class="p">}</span>
        <span class="p">}</span>

        <span class="nf">Ok</span><span class="p">(</span><span class="n">Tensor</span> <span class="p">{</span>
            <span class="n">data</span><span class="p">:</span> <span class="n">new_data</span><span class="p">,</span>
            <span class="n">shape</span><span class="p">:</span> <span class="n">transposed_shape</span><span class="nf">.to_vec</span><span class="p">(),</span>
            <span class="n">strides</span><span class="p">:</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_strides_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="n">transposed_shape</span><span class="p">),</span>
            <span class="n">track_grad</span><span class="p">:</span> <span class="k">true</span><span class="p">,</span>
            <span class="n">create_op</span><span class="p">:</span> <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="n">Transpose</span><span class="p">),</span>
            <span class="n">derivative</span><span class="p">:</span> <span class="nn">RefCell</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span>
                <span class="mf">0.0</span><span class="p">;</span>
                <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="n">transposed_shape</span><span class="p">)</span>
            <span class="p">]),</span>
            <span class="n">lhs_parent</span><span class="p">:</span> <span class="nf">Some</span><span class="p">(</span><span class="k">self</span><span class="p">),</span>
            <span class="n">rhs_parent</span><span class="p">:</span> <span class="nb">None</span><span class="p">,</span>
        <span class="p">})</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>Now that we have this, let’s add matmul support.</p>

<p>First up, let’s add a BLAS crate to <code class="language-plaintext highlighter-rouge">Cargo.toml</code>. Note that I’m using Apple’s accelerate as the BLAS library backend since its already installed on my Macbook pro, but you can change it to use <a href="https://crates.io/crates/blas-src">another</a> BLAS library if you want.</p>

<hr />

<div class="language-toml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">[dependencies]</span>
<span class="py">blas</span> <span class="p">=</span> <span class="s">"0.20.0"</span>
<span class="nn">blas-src</span> <span class="o">=</span> <span class="p">{</span> <span class="py">version</span> <span class="p">=</span> <span class="s">"0.6"</span><span class="p">,</span> <span class="py">features</span> <span class="p">=</span> <span class="nn">["accelerate"]</span> <span class="p">}</span>
</code></pre></div></div>

<hr />

<p>Let’s add some Ops and errors for matmul</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/ops.rs</span>

<span class="nd">#[derive(Debug,</span> <span class="nd">PartialEq)]</span>
<span class="k">pub</span> <span class="k">enum</span> <span class="n">Ops</span> <span class="p">{</span>
    <span class="o">...</span>
    <span class="n">Matmul</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Display</span> <span class="k">for</span> <span class="n">Ops</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">fmt</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Formatter</span><span class="o">&lt;</span><span class="nv">'_</span><span class="o">&gt;</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Result</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span> <span class="p">{</span>
            <span class="o">...</span>
            <span class="nn">Ops</span><span class="p">::</span><span class="n">Matmul</span> <span class="k">=&gt;</span> <span class="nd">write!</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"Matmul"</span><span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/errors.rs</span>
<span class="nd">#[derive(Debug,</span> <span class="nd">Clone)]</span>
<span class="k">pub</span> <span class="k">enum</span> <span class="n">TensorError</span> <span class="p">{</span>
    <span class="o">...</span>
    <span class="n">MatmulShapeError</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">impl</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Display</span> <span class="k">for</span> <span class="n">TensorError</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">fmt</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">f</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Formatter</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="nn">fmt</span><span class="p">::</span><span class="n">Result</span> <span class="p">{</span>
        <span class="k">match</span> <span class="k">self</span> <span class="p">{</span>
            <span class="o">...</span>
            <span class="nn">TensorError</span><span class="p">::</span><span class="n">MatmulShapeError</span> <span class="k">=&gt;</span> <span class="nd">write!</span><span class="p">(</span>
                <span class="n">f</span><span class="p">,</span>
                <span class="s">"Tensors must be two dimensions each and must be matrix multipliable"</span>
            <span class="p">),</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>

</code></pre></div></div>

<hr />

<p>Let’s add the matrix multiplication code</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="nd">#[allow(clippy::many_single_char_names)]</span>
    <span class="k">fn</span> <span class="nf">two_dimension_matmul</span><span class="p">(</span><span class="n">lhs</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">rhs</span><span class="p">:</span> <span class="o">&amp;</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">out</span><span class="p">:</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f32</span><span class="o">&gt;</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">let</span> <span class="n">lhs</span> <span class="o">=</span> <span class="n">lhs</span><span class="nf">.transpose</span><span class="p">()</span><span class="nf">.unwrap</span><span class="p">();</span>
        <span class="k">let</span> <span class="n">rhs</span> <span class="o">=</span> <span class="n">rhs</span><span class="nf">.transpose</span><span class="p">()</span><span class="nf">.unwrap</span><span class="p">();</span>

        <span class="k">let</span> <span class="n">a</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f64</span><span class="o">&gt;</span> <span class="o">=</span> <span class="n">lhs</span><span class="py">.data</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.map</span><span class="p">(|</span><span class="n">val</span><span class="p">|</span> <span class="o">*</span><span class="n">val</span> <span class="k">as</span> <span class="nb">f64</span><span class="p">)</span><span class="nf">.collect</span><span class="p">();</span>
        <span class="k">let</span> <span class="n">b</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f64</span><span class="o">&gt;</span> <span class="o">=</span> <span class="n">rhs</span><span class="py">.data</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.map</span><span class="p">(|</span><span class="n">val</span><span class="p">|</span> <span class="o">*</span><span class="n">val</span> <span class="k">as</span> <span class="nb">f64</span><span class="p">)</span><span class="nf">.collect</span><span class="p">();</span>

        <span class="k">let</span> <span class="k">mut</span> <span class="n">c</span><span class="p">:</span> <span class="nb">Vec</span><span class="o">&lt;</span><span class="nb">f64</span><span class="o">&gt;</span> <span class="o">=</span>
            <span class="nd">vec!</span><span class="p">[</span><span class="mf">0.0</span><span class="p">;</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="p">[</span><span class="n">lhs</span><span class="py">.shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                <span class="n">rhs</span><span class="py">.shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])];</span>

        <span class="k">let</span> <span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">lhs</span><span class="py">.shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">as</span> <span class="nb">i32</span><span class="p">,</span>
            <span class="n">rhs</span><span class="py">.shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">as</span> <span class="nb">i32</span><span class="p">,</span>
            <span class="n">lhs</span><span class="py">.shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">as</span> <span class="nb">i32</span><span class="p">,</span>
        <span class="p">);</span>

        <span class="k">unsafe</span> <span class="p">{</span>
            <span class="nf">dgemm</span><span class="p">(</span><span class="n">b</span><span class="sc">'N'</span><span class="p">,</span> <span class="n">b</span><span class="sc">'N'</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">a</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">b</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="n">c</span><span class="p">,</span> <span class="n">m</span><span class="p">);</span>
        <span class="p">}</span>

        <span class="k">let</span> <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="nf">.iter</span><span class="p">()</span><span class="nf">.map</span><span class="p">(|</span><span class="n">val</span><span class="p">|</span> <span class="o">*</span><span class="n">val</span> <span class="k">as</span> <span class="nb">f32</span><span class="p">)</span><span class="nf">.collect</span><span class="p">();</span>
        <span class="k">let</span> <span class="n">c</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="o">&amp;</span><span class="p">[</span><span class="n">rhs</span><span class="py">.shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">lhs</span><span class="py">.shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span><span class="nf">.unwrap</span><span class="p">();</span>
        <span class="k">let</span> <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="nf">.transpose</span><span class="p">()</span><span class="nf">.unwrap</span><span class="p">();</span>

        <span class="k">let</span> <span class="k">mut</span> <span class="n">c</span> <span class="o">=</span> <span class="n">c</span><span class="py">.data</span><span class="p">;</span>

        <span class="n">out</span><span class="nf">.append</span><span class="p">(</span><span class="o">&amp;</span><span class="k">mut</span> <span class="n">c</span><span class="p">);</span>
    <span class="p">}</span>

    <span class="k">pub</span> <span class="k">fn</span> <span class="nf">matmul</span><span class="p">(</span><span class="o">&amp;</span><span class="nv">'a</span> <span class="k">self</span><span class="p">,</span> <span class="n">rhs</span><span class="p">:</span> <span class="o">&amp;</span><span class="nv">'a</span> <span class="n">Tensor</span><span class="p">)</span> <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
        <span class="k">let</span> <span class="n">new_shape</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">validate_tensors</span><span class="p">(</span><span class="k">self</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">rhs</span><span class="p">)</span><span class="o">?</span><span class="p">;</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">new_shape</span><span class="nf">.len</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">)</span> <span class="p">||</span> <span class="p">(</span><span class="n">new_shape</span><span class="nf">.len</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
            <span class="k">return</span> <span class="nf">Err</span><span class="p">(</span><span class="nn">TensorError</span><span class="p">::</span><span class="n">MaxDimsError</span><span class="p">);</span>
        <span class="p">}</span>

        <span class="k">let</span> <span class="k">mut</span> <span class="n">new_data</span> <span class="o">=</span> <span class="nn">Vec</span><span class="p">::</span><span class="nf">with_capacity</span><span class="p">(</span><span class="nn">Tensor</span><span class="p">::</span>
            <span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="n">new_shape</span><span class="p">));</span>

        <span class="k">if</span> <span class="n">new_shape</span><span class="nf">.len</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="p">{</span>
            <span class="nn">Tensor</span><span class="p">::</span><span class="nf">two_dimension_matmul</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">,</span> <span class="n">rhs</span><span class="p">,</span> <span class="o">&amp;</span><span class="k">mut</span> <span class="n">new_data</span><span class="p">)</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="k">return</span> <span class="nf">Err</span><span class="p">(</span><span class="nn">TensorError</span><span class="p">::</span><span class="n">MatmulShapeError</span><span class="p">);</span>
        <span class="p">}</span>

        <span class="nf">Ok</span><span class="p">(</span><span class="n">Tensor</span> <span class="p">{</span>
            <span class="n">data</span><span class="p">:</span> <span class="n">new_data</span><span class="p">,</span>
            <span class="n">shape</span><span class="p">:</span> <span class="n">new_shape</span><span class="nf">.to_vec</span><span class="p">(),</span>
            <span class="n">strides</span><span class="p">:</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">calc_strides_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="n">new_shape</span><span class="p">),</span>
            <span class="n">track_grad</span><span class="p">:</span> <span class="k">true</span><span class="p">,</span>
            <span class="n">create_op</span><span class="p">:</span> <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="n">Matmul</span><span class="p">),</span>
            <span class="n">derivative</span><span class="p">:</span> <span class="nn">RefCell</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mf">0.0</span><span class="p">;</span> <span class="nn">Tensor</span><span class="p">::</span>
                <span class="nf">calc_tensor_len_from_shape</span><span class="p">(</span><span class="o">&amp;</span><span class="n">new_shape</span><span class="p">)]),</span>
            <span class="n">lhs_parent</span><span class="p">:</span> <span class="nf">Some</span><span class="p">(</span><span class="k">self</span><span class="p">),</span>
            <span class="n">rhs_parent</span><span class="p">:</span> <span class="nf">Some</span><span class="p">(</span><span class="n">rhs</span><span class="p">),</span>
        <span class="p">})</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>Now that we have that, let’s add autograd support for matmul</p>

<hr />

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/tensor.rs</span>
<span class="k">impl</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="n">Tensor</span><span class="o">&lt;</span><span class="nv">'a</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="k">fn</span> <span class="nf">grad</span><span class="p">(</span><span class="o">&amp;</span><span class="k">self</span><span class="p">)</span> <span class="p">{</span>

        <span class="k">if</span> <span class="k">let</span> <span class="nf">Some</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">=</span> <span class="k">self</span><span class="py">.lhs_parent</span> <span class="p">{</span>
            <span class="k">let</span> <span class="n">d_lhs</span> <span class="o">=</span> <span class="k">match</span> <span class="o">&amp;</span><span class="k">self</span><span class="py">.create_op</span> <span class="p">{</span>
                <span class="o">...</span>
                <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="n">Matmul</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="k">self</span><span class="py">.rhs_parent</span><span class="nf">.unwrap</span><span class="p">()</span><span class="nf">.transpose</span><span class="p">(),</span>
            <span class="p">}</span>
            <span class="nf">.unwrap</span><span class="p">();</span>

            <span class="k">let</span> <span class="n">d_lhs</span> <span class="o">=</span> <span class="k">match</span> <span class="k">self</span><span class="py">.create_op</span> <span class="p">{</span>
                <span class="o">...</span>
                <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="n">Matmul</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="n">d</span><span class="nf">.matmul</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d_lhs</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">(),</span>
                <span class="mi">_</span> <span class="k">=&gt;</span> <span class="o">&amp;</span><span class="n">d_lhs</span> <span class="o">*</span> <span class="o">&amp;</span><span class="n">d</span><span class="p">,</span>
            <span class="p">};</span>
        <span class="p">}</span>

        <span class="k">if</span> <span class="k">let</span> <span class="nf">Some</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">=</span> <span class="k">self</span><span class="py">.rhs_parent</span> <span class="p">{</span>
            <span class="k">let</span> <span class="n">d_rhs</span> <span class="o">=</span> <span class="k">match</span> <span class="k">self</span><span class="py">.create_op</span> <span class="p">{</span>
                <span class="o">...</span>
                <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="n">Matmul</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="k">self</span><span class="py">.lhs_parent</span><span class="nf">.unwrap</span><span class="p">()</span><span class="nf">.transpose</span><span class="p">(),</span>
            <span class="p">}</span>
            <span class="nf">.unwrap</span><span class="p">();</span>

            <span class="k">let</span> <span class="n">d_rhs</span> <span class="o">=</span> <span class="k">match</span> <span class="k">self</span><span class="py">.create_op</span> <span class="p">{</span>
                <span class="o">...</span>
                <span class="nf">Some</span><span class="p">(</span><span class="nn">Ops</span><span class="p">::</span><span class="n">Matmul</span><span class="p">)</span> <span class="k">=&gt;</span> <span class="n">d_rhs</span><span class="nf">.matmul</span><span class="p">(</span><span class="o">&amp;</span><span class="n">d</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">(),</span>
            <span class="p">};</span>

            <span class="k">let</span> <span class="n">d_rhs_prev</span> <span class="o">=</span>
                <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="n">t</span><span class="py">.derivative</span><span class="nf">.borrow</span><span class="p">()</span><span class="nf">.clone</span><span class="p">(),</span> <span class="o">&amp;</span><span class="n">t</span><span class="py">.shape</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">();</span>
            <span class="k">let</span> <span class="n">d_rhs</span> <span class="o">=</span> <span class="o">&amp;</span><span class="n">d_rhs</span> <span class="o">+</span> <span class="o">&amp;</span><span class="n">d_rhs_prev</span><span class="p">;</span>
            <span class="o">*</span><span class="n">t</span><span class="py">.derivative</span><span class="nf">.borrow_mut</span><span class="p">()</span> <span class="o">=</span> <span class="n">d_rhs</span><span class="py">.data</span><span class="p">;</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<hr />

<p>Let’s try it out:</p>

<div class="language-rust highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">// src/main.rs</span>
<span class="k">fn</span> <span class="nf">main</span><span class="p">()</span> <span class="k">-&gt;</span> <span class="n">Result</span><span class="o">&lt;</span><span class="p">(),</span> <span class="n">TensorError</span><span class="o">&gt;</span> <span class="p">{</span>
    <span class="k">let</span> <span class="n">a</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">],</span> <span class="o">&amp;</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span><span class="nf">.unwrap</span><span class="p">();</span>
    <span class="k">let</span> <span class="n">b</span> <span class="o">=</span> <span class="nn">Tensor</span><span class="p">::</span><span class="nf">new</span><span class="p">(</span><span class="nd">vec!</span><span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">],</span> <span class="o">&amp;</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span><span class="nf">.unwrap</span><span class="p">();</span>

    <span class="k">let</span> <span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="nf">.matmul</span><span class="p">(</span><span class="o">&amp;</span><span class="n">b</span><span class="p">)</span><span class="nf">.unwrap</span><span class="p">();</span>

    <span class="n">c</span><span class="nf">.backward</span><span class="p">();</span>

    <span class="nd">println!</span><span class="p">(</span><span class="s">"{}"</span><span class="p">,</span> <span class="n">c</span><span class="p">);</span>

    <span class="nf">Ok</span><span class="p">(())</span>
<span class="p">}</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bilal@Bilals-MacBook-Pro l2 % cargo run
   Compiling l2 v0.1.0 <span class="o">(</span>/Users/bilal/Desktop/l2<span class="o">)</span>
    Finished dev <span class="o">[</span>unoptimized + debuginfo] target<span class="o">(</span>s<span class="o">)</span> <span class="k">in </span>1.46s
     Running <span class="sb">`</span>target/debug/main<span class="sb">`</span>

Value: <span class="o">[</span>22.0, 28.0, 49.0, 64.0]
Shape: <span class="o">[</span>2, 2]
Lhs:
  Value: <span class="o">[</span>1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
  Shape: <span class="o">[</span>2, 3]
  Lhs: None
  Rhs: None
  Op: None
  TrackGrad: <span class="nb">true
  </span>Derivative: <span class="o">[</span>3.0, 7.0, 11.0, 3.0, 7.0, 11.0]
Rhs:
  Value: <span class="o">[</span>1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
  Shape: <span class="o">[</span>3, 2]
  Lhs: None
  Rhs: None
  Op: None
  TrackGrad: <span class="nb">true
  </span>Derivative: <span class="o">[</span>5.0, 5.0, 7.0, 7.0, 9.0, 9.0]
Op: Matmul
TrackGrad: <span class="nb">true
</span>Derivative: <span class="o">[</span>1.0, 1.0, 1.0, 1.0]
</code></pre></div></div>

<hr />

<p>Well thats pretty much it for the first draft. Ill see about adding more stuff when I redo this whole post.</p>

<hr />

<h1 id="future-work">Future Work</h1>

<hr />

<ul>
  <li>rust arrays vs vec
    <ul>
      <li>const generics</li>
    </ul>
  </li>
  <li>jax</li>
  <li>compiler in rust</li>
</ul>

<hr />

<h1 id="conclusions">Conclusions</h1>

<hr />

<p><em>todo</em></p>

<ul>
  <li>benchmarks</li>
  <li>subsections</li>
  <li>gradient vs derivative</li>
  <li>standardize code snippets</li>
  <li>move implementing ops to beginning</li>
  <li>naive matmul</li>
  <li>slicing?</li>
</ul>

<hr />

<h1 id="resources">Resources</h1>

<hr />

<hr />

<h1 id="references">References</h1>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>I guess the fact that I like to spend my last free summer working on a side project says a lot about me :p <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>I’m almost certain that there are a few bugs in how I handle backpropogation through broadcasted tensors <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>That’s the summer of 2019, for those of you reading this in the near or not so near future :) <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>https://blog.ezyang.com/2019/05/pytorch-internals/’&gt;http://blog.ezyang.com/2019/05/pytorch-internals <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>In my defense, I was pretty bad at algorithmy stuff back then <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Bilal  Khan.
    <a href='https://bilal2vec.github.io'>https://bilal2vec.github.io</a>

    
    
    Last updated: June 21, 2021.
    
  </div>
</footer>



  </body>

  <d-bibliography src="/blog/assets/bibliography/2020-08-02-rust-ml-library.bib">
  </d-bibliography>

</html>
