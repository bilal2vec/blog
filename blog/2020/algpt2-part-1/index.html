<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Bilal Khan | ALGPT2 Part 1</title>
<meta name="description" content="Bilal's Blog
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤–</text></svg>">

<link rel="stylesheet" href="/blog/assets/css/main.css">
<link rel="canonical" href="/blog/blog/2020/algpt2-part-1/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/blog/assets/js/theme.js"></script>
<script src="/blog/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    <script src="/blog/assets/js/distillpub/template.v2.js"></script>
    <script src="/blog/assets/js/distillpub/transforms.v2.js"></script>
    
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
    
  </head>

  <d-front-matter>
    <script async type="text/json">{
      "title": "ALGPT2 Part 1",
      "description": "Best Practices for Finetuning Large Transformer Language models",
      "published": "June 22, 2020",
      "authors": [
        
        {
          "author": "Bilal Khan",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://bilal2vec.github.io/blog/">
       Bilal Khan
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/blog/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/blog/uses">
                /Uses
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="post distill">

      <d-title>
        <h1>ALGPT2 Part 1</h1>
        <p>Best Practices for Finetuning Large Transformer Language models</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <p>A few months ago I started working on a research project on how to best finetune GPT2-like language models for text generation. Once I ran a few experiments on that, I wanted to expand the scope of the project and try to pretrain my own, more efficient language model from scratch. I got access to a 128-core TPUv3 pod from the Tensorflow Reseach Cloud and used it to pretrain GPT2-124M to a perplexity pretty close to OpenAIâ€™s results (my pretrained model used was trained for about $1/8$th of the number of iterations that OpenAI trained their model for and got $21$ ppl on OpenWebText compared to $17$ ppl for OpenAIâ€™s model), and then pretrained an ALBERT-style GPT2 (that Iâ€™m calling ALGPT2) language model with a factorized input embedding and parameter sharing that would reduce the number of paramters from 124M to around 12M.</p>

<p>Unfortunately, ALGPT2 doesnâ€™t generate coherent, natural sounding text as well as GPT2 (ALGPT2 gets $31$ ppl on OpenWebText compared to $21$ ppl for my pretrained GPT2 model), but Iâ€™m writing this series of blog posts to go through everything Iâ€™ve learned over the last few months.</p>

<p>I have a cleaned-up version of my codebase on Github <a href="https://github.com/bilal2vec/lm-training-research-project">here</a>, and my original codebase with all my notes <a href="https://github.com/bilal2vec/lm-finetuning">here</a>.</p>

<p>You can take a look at my Weights&amp;Biases workspace with all my runs <a href="https://app.wandb.ai/bilal2vec/lm-finetuning">here</a>.</p>

<h2 id="objectives">Objectives</h2>

<p>I donâ€™t usually have access to a lot of compute (I mostly just use Google Colab) so I started out by limiting the scope of my project to finetuning or running inference on GPT2. I wrote down a <a href="https://github.com/bilal2vec/lm-finetuning/blob/master/Markdown/RESEARCH.md#objectives">few notes</a> on what I wanted to look into:</p>

<ul>
  <li>If I wanted to finetune a LM to generate text of a specific style/content, what good defaults would I choose?</li>
  <li>Find best practices or good defaults for finetuning tranformer language models for text generation.</li>
  <li>Understand the effect of context len, model, and dataset size on generating coherent text</li>
</ul>

<h2 id="data">Data</h2>

<p>I ran most of the finetuning experiments on WikiText-2, which was small enough (~10mb on disk with a total of ~2m words) that I could run experiments fast enough (usually within 5-10m) on the v100 or p100 that I usually got through Colab.</p>

<p>I also ran a few experiments using WikiText-103 (~500mb, 100m words) but these were a lot harder to do because the size of the dataset forced me to use smaller batch sizes which took too long.</p>

<p>Loading in larger datasets (like WikiText-103) into memory can become pretty inefficient because of Pythonâ€™s overhead. IIRC, if you want to load the <em>entire</em> WikiText-103 train set into memory with Python and tokenize the whole thing in one go using Huggingfaceâ€™s Tokenizers library and the GPT2 byte-level BPE tokenizer, it takes about 10 minutes and uses up about 60GB of RAM (Most of the time is spent tokenizing and most of the RAM is used loading the file into memory). Using something like <a href="https://arrow.apache.org/">Apache Arrow</a> like the <a href="https://github.com/huggingface/nlp">Huggingface NLP</a> library should make this a whole lot more efficient.</p>

<p>The WikiText datasets are stored as a single text file, with one Wikipedia article per line. Another more efficient way of processing the data would be to load in the file line-by-line and tokenize each line in parallel using the Huggingface tokenizer libraryâ€™s <code class="language-plaintext highlighter-rouge">batch_encode_plus()</code> function. This is a lot faster and efficient (taking up only 2GB of RAM and 2 minutes) but has its own drawbacks. <code class="language-plaintext highlighter-rouge">batch_encode_plus()</code> truncates sequences beyond that have more than <code class="language-plaintext highlighter-rouge">context_len</code> tokens and leaves sequences that are smaller than <code class="language-plaintext highlighter-rouge">context_len</code> as is, which means that you need to zero-pad the sequences that are smaller than <code class="language-plaintext highlighter-rouge">context_len</code> and discard any portion of a sequence beyond the first <code class="language-plaintext highlighter-rouge">context_len</code> tokens. For datasets that are used to benchmark the performance of a wide range of language models, this can lead to your model being harder to compare against other models that follow the commonly used convention of just tokenizing the entire dataset and chunking it into sequences of <code class="language-plaintext highlighter-rouge">context_len</code> length.</p>

<p>I wanted to make sure the way that I preprocessed the data made sure that the models that I finetuned on WikiText-2 and WikiText-103 would be comparable to other models, so in my code, I load in the entire dataset, tokenize it, and split it into contigous sequences of length <code class="language-plaintext highlighter-rouge">context_len</code>. There are a few other preprocessing-related factors that can affect how comparable results between different models can be, I <a href="/blog/2020/5/14/evaluating-language-models/">wrote a post on the topic</a> a while ago, check it out if youâ€™re interested.</p>

<h2 id="frameworks">Frameworks</h2>

<p>I originally wrote all my code in vanilla Pytorch. I wanted to try using Colabâ€™s free TPUv3 board that has 8 TPU cores each with 16GB of RAM, each of which is a little slower that a single V100. Using the entire TPU board should be <em>at least</em> as fast as using a cluster of 8 V100s but at a much lower cost.</p>

<p>I tried using <a href="https://github.com/PyTorchLightning/pytorch-lightning">Pytorch Lightning</a> to see if it would help make the Pytorch model run on the Colab TPU more easily, but after about a week of trying to use the library I switched over to having two training scripts; One in plain Pytorch and one in Tensorflow2.0 with Keras â€” Even though Pytorch Lightning was very well designed, the time and effort required to make sure the framework is working the way that I want wasnâ€™t worth it in the end.</p>

<p>This was my first time working with TF/Keras since around early 2018 when I switched to Pytorch (back in the 0.3 days when you still had to use <code class="language-plaintext highlighter-rouge">Variable</code>). TF2.0 is a lot better now than it used to be two or three years ago but still doesnâ€™t feel as intuitive or easy to use as Pytorch. The documentation looks pretty good at first glance but there were a lot of gaps in the documentation when I was trying to figure out how to write and decode TFRecord files and scale my Keras code to TPUs and TPU pods.</p>

<blockquote>
  <p>Trying to get gradient accumulation to work with TPUs was especially hard, IIRC grad accumulation isnâ€™t natively supported in Keras but there are a lot of independent implementations that people have open-sourced on Github, but they didnâ€™t work well with TPUs.</p>
</blockquote>

<p>I used the <a href="https://github.com/huggingface/transformers">Huggingface/Transformers</a> repository for the GPT-2 model and <a href="https://www.wandb.com/">Weights&amp;Biases</a> to track all the experiments that I ran.</p>

<blockquote>
  <p>Fun fact, I ran into a problem with Colabâ€™s TPU a couple of times where I was silently downgraded from a TPUv3 to a TPUv2 and as a result I was getting a lot of OOM errors for a model and batch size that was working perfectly just a few hours ago. Colab doesnâ€™t really advertise this and makes it almost impossible to know if you have been downgraded :(</p>
</blockquote>

<p>Pytorch recently released <a href="https://github.com/pytorch/xla">Pytorch/XLA</a> which is supposed to let you run Pytorch code on TPUs with only a few changes to your code. I spent quite a bit of time to try and make this work but using it is still a lot more complex than just using a GPU.</p>

<p>Pytorch/XLA is a lot slower on Colab, which probably has something to do with Colabâ€™s network connection to the TPUs being a lot slower. Using <a href="https://github.com/pytorch/xla/issues/1777">some operations</a> that arenâ€™t supported by Pytorch/XLA can have a pretty drastic impact on the speed of your program, so if your code is running unusually slow on a TPU, unsupported ops are a common culprit. <a href="https://github.com/pytorch/xla/issues/1777">For example</a>, I was trying to use the memory-saving Adafactor optimizer on Pytorch/XLA but since I was using a non-Pytorch operation in one part of the code (using Pythonâ€™s <code class="language-plaintext highlighter-rouge">sqrt()</code> function instead of <code class="language-plaintext highlighter-rouge">torch.sqrt()</code>), a single iteration was taking ~10 seconds compared to 10 iterations/second for SGD.</p>

<p>TPU support for Pytorch works pretty differently from TPU support for Tensorflow. Each TPU has a powerful dedicated CPU and several 100GBs of RAM for data processing, so whenever you run TF code on a TPU, your data gets copied to each coreâ€™s CPU (unless you use TFRecord files, in which case each coreâ€™s CPU downloads and processes data directly from your cloud bucket) and gets processed there. By doing it in this way, you only need to rent a small cloud instance (like a n1-standard-1) and scaling your code from a single TPU board with 8 cores to a part of a TPU pod is (relatively) painless.</p>

<p>On the other hand, Pytorch/XLA canâ€™t <a href="https://github.com/pytorch/xla/issues/1742">currently use</a> the TPUâ€™s CPU and instead has to replicate the data $8$ times on your own VM for an $8$ core TPU board. If you want to use Pytorch/XLA for a TPU pod, you have to create a VM group with one host VM for each $8$ core TPU board. This means that Pytorch/XLA isnâ€™t currently practical for large scale training, but it <a href="https://github.com/pytorch/xla/issues/1858">looks like</a> the next version of TPUs will be a lot more optimized for Pytorch. It works alright for a small dataset like WikiText-2 but when I tried finetuning on WikiText-103 (~500mb, 100m words) I needed to upgrade my VM to have 80+ GB of RAM.</p>

<h2 id="finetuning">Finetuning</h2>

<p>I wasnâ€™t able to finetune GPT2-1.5b on a TPU with the AdamW optimizer even with the TPUâ€™s built in bfloat16 support, so most of the experiments that I ran were with the memory-saving Adafactor optimizer with <code class="language-plaintext highlighter-rouge">beta1</code> set to zero to disable momentum. Enabling momentum might increase the performance of the Adafactor optimizer, but would also require storing an extra momentum value for each parameter and would make it harder to train larger models.</p>

<p><em>Fun fact: The AdamW optimizer implementation in Googleâ€™s official BERT <a href="https://github.com/google-research/bert/blob/master/optimization.py#L65">repository</a> excludes layernorm and bias parameters from weight decay and AFAICT is the only optimizer that does so. I tried running a few experiments with and without finetuning these parameters and didnâ€™t find any significant difference in performance.</em></p>

<p>Most of the GPU experiments I did were with NVidiaâ€™s Apex library, with its $01$ mixed precision setting. I also tried running a few experiments on using only FP16, but the gradients would explode or vanish and the model wouldnâ€™t train.</p>

<p>I have a <a href="https://github.com/bilal2vec/transformers/tree/grad-checkpointing">forked version</a> of Huggingfaceâ€™s Transformers repository where Iâ€™ve implemented gradient checkpointing for GPT-2. I havenâ€™t been maintaining it but you can see all the changes that I did to make it work (itâ€™s really only a few lines of code) <a href="https://github.com/huggingface/transformers/compare/master...bilal2vec:grad-checkpointing">here</a>. I tried training GPT2-XL with grad checkpointing which IIRC worked with a smaller context length of 256 but still threw OOM errors when finetuning at a context length of 1024.</p>

<p>For small datasets like WikiText-2, (WikiText-2 consists of about 2 million words, so itâ€™s actually on the larger size for datasets that you might collect yourself) the model usually overfits within the first 1-3 epochs, so most of the experiments that I did trained for a single epoch â€” there really is no performance benefit to finetuning for any longer. I set the learning rate for all of my finetuning experiments to $5e-5$ (This was just the first value I tried, no hyperparameter tuning involved) and linearly increased the learning rate from $0$ to $5e-5$ over the first 10% of the training iterations and then linearly decayed it to zero over the rest of the training iterations.</p>

<blockquote>
  <p>Note: The Adafactor paper shows that warmup is strongly recommended to stabilize training, take a look at the <a href="https://arxiv.org/abs/1804.04235">paper</a> for more details</p>
</blockquote>

<p>If you want to finetune GPT2 on a dataset like WikiText-2, thereâ€™s a relationship between the batch size, learning rate, and the number of training iterations that you need to adjust to train effectively and avoid overfitting or plateauing. Thereâ€™s a pretty important ratio that you need to keep constant between the batch size and the learning rate. A larger batch size means that there are fewer gradient updates performed if you keep the number of training iterations constant.</p>

<p>I have a <a href="https://app.wandb.ai/bilal2vec/lm-finetuning/reports/1-epoch-context-len--Vmlldzo3NTI4MA">WandB report</a> showing a few different training runs on WikiText-2 with different sized GPT2 models and context lengths. The results arenâ€™t really that surprising, finetuning larger models at larger context lengths increases perplexity significantly.</p>

<p>I wrote a quick <a href="https://colab.research.google.com/drive/1Vxh91ASFvCPgBL0I6ui97SyxtoLBvY3I?usp=sharing">Colab notebook</a> on how to finetune and evaluate on WikiText-2.</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Bilal  Khan.
    <a href='https://bilal2vec.github.io'>https://bilal2vec.github.io</a>

    
    
    Last updated: June 21, 2021.
    
  </div>
</footer>



  </body>

  <d-bibliography src="/blog/assets/bibliography/2020-06-22-algpt2-part-1.bib">
  </d-bibliography>

</html>
